{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Start time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Start-Time\n",
            "2024-09-25 15:48:29\n",
            "------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from utils import print_time\n",
        "\n",
        "print_time.print_(\"Start-Time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "epochs = 30\n",
        "batch_size = 16\n",
        "weight_decay = 0.01\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 1000\n",
        "metric_for_best_model = \"f1\"\n",
        "early_stopping_patience = 6\n",
        "max_length = 2048\n",
        "\n",
        "hyperparameters = {\n",
        "    'epochs': epochs,     # 1. Baseline\n",
        "    'batch_size': batch_size,\n",
        "    'weight_decay': weight_decay,\n",
        "    'learning_rate': learning_rate,\n",
        "    'warmup_steps': warmup_steps,\n",
        "    'metric_for_best_model': metric_for_best_model,\n",
        "    'early_stopping_patience': early_stopping_patience,\n",
        "    'max_length': max_length,\n",
        "    'use_weighted_loss': False\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_checkpoint = 'mrm8488/longformer-base-4096-spanish-finetuned-squad'\n",
        "# model_checkpoint = 'state-spaces/mamba2-130m'\n",
        "model_checkpoint = 'Narrativa/legal-longformer-base-4096-spanish'\n",
        "# model_checkpoint = 'distilbert-base-uncased'\n",
        "# model_checkpoint = 'roberta-base'\n",
        "# model_checkpoint = 'bert-large-uncased'\n",
        "# model_checkpoint = 'xlnet-base-cased'\n",
        "# model_checkpoint = 'xlnet-large-cased'\n",
        "# model_checkpoint = 'xlm-roberta-large'\n",
        "# model_checkpoint = 'microsoft/deberta-v2-xxlarge'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus_path='corpus/cleaned_corpus.csv'\n",
        "# df = pd.read_csv(corpus_path, sep='\\t', usecols=['Contenido Txt', 'Resultado binario de la acción'])\n",
        "df = pd.read_csv(corpus_path, usecols=['text', 'label'])\n",
        "\n",
        "# rename columns\n",
        "# df.rename(columns = {'Contenido Txt':'text', 'Resultado binario de la acción':'label'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Separate the entries with label 1\n",
        "# df_label_1 = df[df['label'] == 1]\n",
        "\n",
        "# # Randomly sample the same number of entries from label 0\n",
        "# df_label_0 = df[df['label'] == 0].sample(n=len(df_label_1), random_state=42)\n",
        "\n",
        "# # Combine both balanced subsets\n",
        "# df = pd.concat([df_label_1, df_label_0])\n",
        "\n",
        "# # Shuffle the combined DataFrame to mix label 0 and 1\n",
        "# df = df.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cut df to X rows\n",
        "# df = df[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      0  1\\nEXPEDIENTE: RECURSO EXTRAORDINARIO DE\\nCASA...\n",
            "1      0  EXPEDIENTE: RECURSO EXTRAORDINARIO DE CASACIÓN...\n",
            "2      0   lowe Oc\\nCORTE EXPTE: NESTOR VALENTIN GONZALE...\n",
            "3      0  EXPEDIENTE: RECURSO EXTRAORDINARIO DE\\nrE cx C...\n",
            "4      0  EXPEDIENTE: RECURSO DE CASACION INTERPUESTO\\nP...\n"
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "YEGROS Y OTROS s DEFRAUDACION, i\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Yy\n",
            ". \n",
            "CUERDO Y SENTENCIA NUMERO: Noyecventos, sesenta 4 ocho. \n",
            "ta\n",
            "En aAggincién del Paraguay, a los... tyece, seeeseees del mes!\n",
            "Ee\n",
            "RENO cece del afio dos mil. Yec ... it\n",
            "ig reunidos en la Sala de Acuerdos los Excelentisimos\n",
            "Doctores Alicia Beatriz Pucheta de Correa, Sindulfo Blanco y José\n",
            "Ratl Torres K., quien integra la Sala Penal en reemplazo del Dr.\n",
            "Wildo Rienzi Galeano, por ante mi, el Secretaria Autorizante, ser\n",
            "trajo para acuerdo el expediente caratulado: \"RECURSO ;\n",
            "EXTRAORDINARIO DE CASACION INTERPUESTO POR EL SR. HANS FRIEDICH \n",
            "SCHUCHARDT en la causa: IVAN YEGROS Y OTROS S DEFRAUDACION, !\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y OTROS , a fin de \n",
            "resolver el recurso extraordinario de casacién interpuesto por ei!\n",
            "SR. HANS FRIEDICH SCHUCHARDT por derecho propio y bajo patrociniol\n",
            "del Abogado Fabio Cuevas Storm en contra de la S.D. N 22 de,\n",
            "fecha 25 de octubre de 1999 dictada por el Juzgado de Primera \n",
            "Instancia en lo Criminal del Noveno Turno y contra el Acuerdo ys\n",
            "Sentencia N 136 de fecha 13 de diciembre del 2004, dictada por\n",
            "el Tribunal de Apelaciones en lo Penal, 12. Sala de la;\n",
            "t\n",
            "Circunscripcién Judicial de la Capital. Sorc r cer r crs rts rcce (\n",
            "Previo estudio de los antecedentes del caso, la Corte \n",
            "Suprema de Justicia, Sala Penal, resolvié plantear y votar las,\n",
            "Sigulentes: rere rrr nnn nn nena It\n",
            "Cc UESTIONES:\n",
            "1 ) gEs admisible para su estudio el recurso de casacién\n",
            "interpuesto?; \n",
            "2 ) En su caso, jresulta procedente?. \n",
            "A los efectos del andlisis correspondiente de las cuest iones \n",
            "a ser estudiadas y con el objeto de establecer un orden en lat\n",
            "emisién de losg votos, se procede al sorteo, arrojando el \n",
            "siguiente resultadd: Blanco; Pucheta de Correa y Torres \n",
            "ido Levera\n",
            "Abog- 4\n",
            "e aéretario\n",
            "OTROS ) \n",
            "i\n",
            "i\n",
            "i\n",
            "4\n",
            "A la primera cuestién planteada, el Doctor Sindulfo Blanco,\n",
            "dijo: Se presenta el SR. HANS FRIEDICH SCHUCHARDT por dérecho\n",
            "propio y bajo patrocinio del Abogado FABIO ARNALDO CUEVAS STORM y\n",
            "presenta recurso extraordinario de casacién contra la S.D. N 22\n",
            "de fecha 25 de octubre de 1999 dictado por el Juzgado de Primera\n",
            "Instancia en lo Criminal del Noveno Turno y el Acuerdo y\n",
            "Sentencia N 136 de fecha 13 de diciembre del 2004 dictado por el\n",
            "Tribunal de Apelacién en lo Criminal Primera Sala. \n",
            "Tomando en consideracién que el recurrente impugna dos\n",
            "resoluciones dictadas en la presente causa, a los efectos de un\n",
            "orden en el analisis de los agravios formulados, corresponde la\n",
            "divisién en su estudio, procediendo en primer término determinar\n",
            "si la resolucién de primera instancia es objetivamente impugnable\n",
            "ante esta instancia y lo pertinente respecto a la resolucién de\n",
            "segunda instancia. n nnn\n",
            "1) Recurso de casacién interpuesto contra la S.D. N 22 de\n",
            "fecha 25 de octubre de 1999 dictada en primera instancia: El\n",
            "articulo 477 del Cédigo Procesal Penal, dispone en relacién\n",
            "al objeto del recurso: Solo podra deducirse el recurso\n",
            "extraordinario de casacién contra las sentencias definitivas\n",
            "el tribunal de apelaciones o contra las sentencias\n",
            "definitivas del tribunal de apelaciones o contra aquellas\n",
            "decisiones de ese tribunal que pongan fin al procedimiento,\n",
            "extingan la accién o la pena, o denieguen la extincidn,\n",
            "conmutacién o suspensidén de la pena Por otro lado, 479 del\n",
            "mismo cuerpo legal dispone: CASACION DIRECTA. Cuando una\n",
            "sentencia de primera instancia pueda ser impugnada por\n",
            "algunos de los motivos establecidos en el articulo anterior,\n",
            "se podrad interponer directamente el recurso extraordinario\n",
            "de casaciOn, nn nnn nnn nnn nn nnn nnn nnn nnn nnn nnn nnn nnn\n",
            "Conforme a la sistematica de la norma ritual, el recurso\n",
            "extraordinario de casacién puede ser impetrado contra\n",
            "resoluciones de primera instancia (taxativamente determinadas) y\n",
            "contra resoluciones de primera instancia, siempre que se\n",
            "verifiquen ciertos presupuestos de admisibilidad y procedencia.\n",
            "La primera de las posibilidades es denominada doctrinariamente\n",
            "como casacién per saltum . La interposicién del recurso de la\n",
            "3\n",
            " \n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE \n",
            "CASACION INTERPUESTO POR EL SR. HANS, \n",
            "PRIEDICH SCHUCHARDT en la causa: Ivan \n",
            "YEGROS Y OTROS S DEFRAUDACION,\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y \n",
            "OTROS : l\n",
            "en cuestién por un Tribunal de Apelacion, interpretando,\n",
            "ario sensu, si la impugnacidn es realizada ante el!\n",
            "En este caso, el recurrente opto por la segunda posibilidad, ,\n",
            "gomo consta en autos, puesto que interpuso el recurso de\n",
            "apelacién especial ante el Tribunal de Apelacién, el que se\n",
            "pronuncié en consecuencia, y contra el que también se alza ell\n",
            "recurrente, por lo que la oportunidad de impugnar la resolucidnf \n",
            "de primera instancia, via casacién directa, a esta altura, esta\n",
            "preclusa, resultando extemporadnea la pretensidn del impugnante, \n",
            "por lo que corresponde su rechazZo. 95 nnn nnn nnn nnn\n",
            "Asimismo, conforme a la norma del art. 477 corresponde el!\n",
            "recurso contra resoluciones definitivas o que pongan fin a \n",
            "procedimiento, dictadas por el Tribunal de Apelacién y no se, ;\n",
            "establece que se puede interponer conjuntamente o en format\n",
            "acumulativa, respecto a resoluciones de primera instancia. \n",
            "Respecto a la interpretacién de la misma, la Sala Penal se hat\n",
            "pronunciado en el Acuerdo y Sentencia N 1644 de fecha 23 del\n",
            "noviembre del 2004: .. En este punto, la norma resulta de por si!\n",
            "suficientemente clarificadora la disponer taxativamente que 1al\n",
            "forma de su interposicién debe limitarse a la impugnacion de 1a\n",
            "sentencia del TRIBUNAL DE APELACIONES , y no como hizo el\n",
            "i\n",
            "recurrente expresar agravios indistintamente contra la Resolucién \n",
            "eh\n",
            "de la Camara y del A-quo. Consecuentemente, tal planteamientd\n",
            "deviene improcedente en cuanto al estudio de los consignadd\n",
            "contra la decisidn recaida en primera instancia, siendd\n",
            "procedente si, entrar a estudiar la viabilidad de los argumentos\n",
            "vertidos en contra de la sentencia del Tribunal de Apelaciones . -\n",
            "no corresponde el estudio del recurs9\n",
            "Consecuentemente,\n",
            "interpuesto con\n",
            "Al Arnaido Levera\n",
            "Secretario\n",
            " \n",
            "a o 5\n",
            "2) Recurso de casacién interpuesto contra el Acuerdo y\n",
            "Sentencia N 136 de fecha 13 de diciembre del 2004 dictada\n",
            "por el Tribunal de Apelacién en lo Penal 1 Sala de la\n",
            "Circunscripeién Judicial de la capital: En efecto, de\n",
            "acuerdo a las constancias de autos, la resolucién recurrida,\n",
            "es un Acuerdo y Sentencia que confirma un fallo de Primera\n",
            "instancia (S.D. N 22 de fecha 25 de octubre de 1999, que\n",
            "impuso a HANS FRIEDICH SCHUCHARDT la pena privativa de\n",
            "libertad de (OCHO ANOS), situacién esta que evidencia el\n",
            "caracter de definitiva y extintiva del procedimiento, por lo\n",
            "que la resolucién recurrida se torna pasible de ser revisada\n",
            "por la via de la casacién impetrada. \n",
            "Por otro lado, conforme a lo dispuesto por el art. 480 del\n",
            "C.P.P., en concordancia con el art. 468 del mismo cuerpo legal,\n",
            "la impugnacién debe interponerse en un plazo de diez dias de\n",
            "notificada la resolucién, lo que en este caso se verifica, puesto\n",
            "que tal como lo afirma el representante del Ministerio Publico no\n",
            "hay constancia de notificacién personal al condenado, conforme lo\n",
            "disponen los articulos 153 y 155 del C.P.P., por lo que se lo\n",
            "tiene por notificado al momento de interponer el recurso\n",
            "respectivo. Por Ultimo, arguye como motivo casacional lo\n",
            "dispuesto por el art. 478 inc. 3 del C.P.P.. enna\n",
            "Ahora bien, el impugnante, present6é ademés a la Sala Penal\n",
            "una peticién de declaracién de prescripcién de la accién penal en\n",
            "la presente causa en fecha 19 de julio del 2007, la cual fue\n",
            "sustanciada corriéndose traslado al representante de la querella\n",
            "como al Ministerio Piblico. nnn nnn nnn nnn nnn\n",
            "En ese sentido, ésta Sala Penal de la Corte Suprema de\n",
            "Justicia de una manera coincidente y constante viene afirmando\n",
            "que toda cuestién que involucre la duracién del proceso penal\n",
            " garantia del plazo razonable debe ser estudiada con\n",
            "preeminencia a cuestiones formales e incluso de fondo, puesto que\n",
            "implica la posibilidad de que la causa iniciada al procesado se\n",
            "haya extendido en el tiempo mas alla de lo que la ley\n",
            "especificamente reglamenta, con lo cual una extensién del mismo\n",
            "constituiria el proceso en arbitrario e ilegal, en afectacion\n",
            "ap\n",
            "J\n",
            "5\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "YEGROS Y OTROS s DEFRAUDACION, !\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS\" \n",
            "mencionado principio. Consecuentemente, de una manera\n",
            "poo ,\n",
            "u\n",
            "Dicho criterio ya ha sido sostenido en los siguientes casos: \n",
            "adiente RECURSO EXTRAORDINARIO DE CASACION INTERPUESTO POR EL!\n",
            "DEFENSOR PUBLICO Abog. CARLOS FLORES CARTES en la causa: \n",
            "CRISTHIAN EVER REYES S ROBO AGRAVADO ; Acuerdo y Sentencia N \n",
            "1474 de fecha 11 de diciembre del 2006 en el RECURSO \n",
            "EXTRAORDINARIO DE CASACION INTERPUESTO POR EL Abog. CARLOS BRITEZ \n",
            "CARDENAS en los autos: ENRIQUE GARCIA Y JOSE LEOPOLDO MENDOZA s \n",
            "exp!\n",
            "H.P.C LA VIDA, HOMICIDIO CULPOSO; Acuerdo y Sentencia N 1154 de;\n",
            "fecha 15 de diciembre del 2005 dictado en la causa: RECURSO;\n",
            "EXTRORDINARIO DE CASACION INTERPUESTO PPOR EL Abog. FAVIO MANUEL,\n",
            "RAMOS VILLASBOA en la causa: OLIMPIO ARAUJO S HOMICIDIO DOLOSO;\n",
            "EN YUTY y Auto Interlocutorio N 1060 de fecha 02 de julio dei!\n",
            "2007 dictado en el expediente RECURSO EXTRAORDINARIO DE CASACION!\n",
            "INTERPUESTO POR LOS AGENTES FISCALES ABOG. ALBA MARIA DELVALLE ral\n",
            "ABOG. RENE FERNANDEZ BOBADILLA EN LA CAUSA: VALERIA SARA \n",
            "MERCEDES ORTIZ DE ESTECHE, NELSON EUGENIO MENDEZ MORINIGO, RAMON\n",
            "A. GUILLEN, DIONISIO CORONEL Y OTROS S LESION DE CONFIANZA \n",
            "ASOCIACION CRIMINAL . 3-9 n ner rrr rr rrr rn 1\n",
            "La defensa sostiene que debe tenerse presente que la causal\n",
            "inicié en el afio 1996 y que es debido considerar que a los \n",
            "efectos legales, es la disposicién del tipo base de la norma por!\n",
            "'\n",
            "la que fue condenado el procesado art. 29 y 192 inc. 2 del C.P.,'\n",
            "I\n",
            "14 de la C.N., 5? inc. 3 del C.P. y 4 del C.P.P. , la que debe!\n",
            "primar. Sostiene, reafirmando sus agravios, que como para el!\n",
            "computo del plazh We duracién de la causa para la prescripcién se\n",
            "Abog. AFnaildo Levera\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO \n",
            "ahteri s al estudio de las peticiones formuladas primigeniamente \n",
            "Aduerdo y Sentencia N 16 de fecha 13 de febrero del 2007 en ell \n",
            "Secretario\n",
            "1d\n",
            "ese. ee SS OS\n",
            " \n",
            "interrupcién y coémputo del doble del plazo de prescripcién, luego\n",
            "de lo cual resulta de acuerdo a sus afirmaciones claro sostener\n",
            "que la causa que se le sigue ha prescripto. 3 nm nna nn\n",
            "Tanto el Ministerio Puiblico como la querella se oponen a la\n",
            "pretensién. Para el 6rgano acusador la etapa procesal para la\n",
            "peticién formulada la precluido. Por otro lado, la querella\n",
            "afirma (fs. 354 362), en cuanto a la extincién de la accién penal\n",
            "que por imperio de la ley 1444 no es posible aplicar lo dispuesto\n",
            "por el art. 136 del C.P.P., en razén en que el presente proceso\n",
            "file sustanciado conforme a las disposiciones de la anterior ley\n",
            "procesal, y no esta permitido integrar normas procesales de una y\n",
            "otra ley, 2922 2-22-22 eee\n",
            "Por otro lado, en cuanto a la prescripcién de la accién\n",
            "penal afirma que el Cédigo Penal no contempla el instituto de la\n",
            "prescripcién de la accién penal, sino el de la sancién penal, por\n",
            "lo cual no es posible declararla en la presente causa. \n",
            "En cuanto a lo alegado por el representante del Ministerio\n",
            "Publico, preclusién de la etapa para solicitar la prescripcidén de\n",
            "la accién penal, la Sala Penal de la Corte Suprema ha sefialado,\n",
            "cémo anteriormente se ha resefiado, que tanto la extincidén de la\n",
            "accién penal, como la prescripcién de la accién puede ser\n",
            "articulada e incluso estudiada de oficio por el s6rgano\n",
            "jurisdiccional en cualquier etapa de la investigacién, puesto que\n",
            "al estar implicado el principio de plazo razonable de\n",
            "juzgamiento, implica una cuestién de orden putblico, en directa\n",
            "aplicacién a lo dispuesto por el art. 17 de la C.N. y art. 8 del\n",
            "Pacto de San José de Costa Rica. 9 nn nnn nnn nn nnn nnn\n",
            "Efectivamente, ante la alegacién similar del Ministerio\n",
            "Piblico preclusién del derecho de peticionar la prescripcién de\n",
            "la accién penal se ha respondido, por los dos Ministros de la\n",
            "Sala Penal que hemos emitido opinién, que lo atinente al plazo\n",
            "razonable su andlisis y pronunciamiento en cualquier etapa del\n",
            "proceso. (Auto Interlocutorio N 1060 de fecha 02 de julio del\n",
            "2007 dictado en el expediente: RECURSO EXTRAORDINARIO BDE\n",
            "CASACION INTERPUESTO POR LOS AGENTES FISCALES ABOG. ALNBA MARIA\n",
            "DELVALLE Y ABOG. RENE FERNANDEZ BOBADILLA EN LA CAUSA: VALERIA\n",
            "SARA MERCEDES ORTIZ DE ESTECHE, NESTOR EUGENIO MENDEZ MORINIGO,\n",
            "7\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. HANS,\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN \n",
            "YEGROS Y OTROS s DEFRAUDACION, \n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS : \n",
            "eek \n",
            "oat AEQSUTLLEN, DIONISIO CORONEL Y OTROS S LESION DE CONFIANZA\n",
            "ee\n",
            "ON va\n",
            "re\n",
            "RY\n",
            "ANGE taérecho de fondo, corresponde pronunciarse sobre la supuesta\n",
            "inexistencia del instituto de la prescripcién de la accién en el\n",
            "SOCLACION CRIMINAL ) . \n",
            "Sra bien, por tratarse de una cuestién sustancial, propia\n",
            "Cédigo Penal y si fuera necesario, posteriormente lo relativo al\n",
            "imstituto procesal de la Extincién de la accién penal. \n",
            "El representante de la querella sostiene que no puede \n",
            "declararse la prescripcién de la accién penal por cuanto la ley \n",
            "penal sustantiva solo contiene la previsién de una prescripcidén;\n",
            "de la sancién penal. En cuanto a ello es menester realizar!\n",
            "ciertas precisiones, 3-95 nnn ene nner errr\n",
            "La doctrina determina que existen ambas especies. La\n",
            "prescripcién de la accién penal que impide la prosecucion de la\n",
            "causa, cuando en la misma, trascurrido los plazos previstos en qa! J\n",
            "ley, no se ha dictado sentencia definitiva firme. Con lo cual la\n",
            "accién presentada no ha llegado a su consecuencia natural: la\n",
            "redefinicién del conflicto a través de una norma juridicas \n",
            "particular aplicable sdédlo a las partes, en otras palabras, unal\n",
            "sentencia judicial . 3-5-9 nnn nnn nnn nnn nnn\n",
            "La prescripcién de la sancién penal por otro lado implica;\n",
            "que una vez dictada una sentencia definitiva de condena, la misma\n",
            "no es cumplida, por el motivo que fuere, con lo cual, aun cuando!\n",
            "se hay redefinido el conflicto, la solucién dispuesta por al\n",
            "érgano jurisdiccional sin embargo no tiene virtualidad practica y\n",
            "va diluyéndose con el trascurso del tiempo. s\n",
            "Igualmente, ha sefialado en relacién a la prescripcién de la pena:\n",
            " .. La prescripcién de a pena no atenta contra el derecho \n",
            "persecutorio a su imposicién o cumplimiento, sino que hace;\n",
            "referencia especifica al incumplimiento de una pena ya\n",
            "establecida. BY jeti i ira- por el instituto de la\n",
            "prescripcio.\n",
            "AbogArnaldo Levera\n",
            "Secretario\n",
            "resolucién en punto a la exigencia de su ejecutoriedad \n",
            "(Sproviero Juan, Prescripcién de la accién y de la pena, p. 42,\n",
            "Editorial Abaco de Rodolfo Desalma, Bs. As. Argentina, 2001).\n",
            "Afirma ademas que el computo del plazo para la prescripcién de la\n",
            "pena se contabiliza a partir de dictado de la Sentencia\n",
            "Definitiva firme, sea que haya sido notificada a la defensa o al\n",
            "condenado en relacién a ello existe una fuerte discusién\n",
            "doctrinaria (Sproviero Juan, ob.cit. P. 34) . \n",
            "En ese orden de cosas, en cuanto al cdmputo de la\n",
            "prescripcién de la sancioén penal, la Jurisprudencia argentina\n",
            "citada por la doctrina sostuvo: ..La prescripcién de la pena se\n",
            "opera por el trascurso el termino de ella (..) ..se debe concluir\n",
            "en que la pena impuesta en sentencia de 2? Instancia conlleva que\n",
            "corra la prescripcién desde que de algtin modo legalmente\n",
            "notificado el encausado y a que el dispositivo adquiere el\n",
            "caraécter de definitivo e irrevocable (Hernandez Carolina, Accidén\n",
            "y Prescripcién en el Derecho Penal, p. 190 1, Editorial Juris,\n",
            "Rosario Argentina, afio 2000) . rn nnn rrr\n",
            "Consecuentemente, presupuestos para la prescripcién de la\n",
            "sancién penal constituyen: 1) que exista una sentencia firme; 2)\n",
            "que la misma no pueda ser ejecutada; 3) el plazo de la\n",
            "prescripcion es igual al trascurso del tiempo de condena sin que\n",
            "la misma haya sido ejecutada,. 9-52 nnn nnn nena\n",
            "Sobre la misma cuestién, la Sala Penal de la Corte Suprema\n",
            "de Justicia, en mayoria ha dicho: ..La prescripcién de la pena\n",
            "suele operar por el trascurso de los lapsos sefialados para cada\n",
            "pena. El plazo prescripto corre desde que al reo se le notifica\n",
            "la sentencia o desde que quebranta la condena. La diferencia\n",
            "fundamental con la prescripcidén de la accién radica en que la de\n",
            "la pena exige que haya habida una sentencia impuesta a un\n",
            "determinado procesado (Acuerdo y -Sentencia N 1144 de fecha 14\n",
            "de diciembre del 2005 dictado en la causa Recurso de Revisién\n",
            "interpuesto por el Abog. Amilcar Ayala Bonzi en los autos\n",
            "caratulados: Paulino Maldonado Silvero y otros s Robo agravado y\n",
            "Reduccion en Ciudad del Este).\n",
            "Abos:\n",
            "9\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. Hans \n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "YEGROS Y OTROS S DEFRAUDACION,\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS yl\n",
            "OTROS\" \n",
            "(Wocigeramente, el Cédigo Penal en el art. 101 establece 1\n",
            "norma che a de que el transcurso del tiempo extingue la sancion ,\n",
            " aay, Sh la cual se configura la previsién de la primera de tas \n",
            "oe de la prescripcién: la de la pena. t\n",
            "Por otro lado, ello no implica que la norma penal no ia!\n",
            "considere o contenga. El art. 102 del mismo cuerpo Legal \n",
            "termina los plazos para la prescripcién de los hechos punibles, 4\n",
            "ighalmente determina el inicio y forma del cdémputo, lo cual\n",
            "claramente alude a la prescripcién de la accién penal, ya que,\n",
            "como se ha visto, la prescripcién de la sancién penal requiere de\n",
            "presupuestos especificos y lo que la ley penal reconocen son\n",
            "presupuestos para la procedencia, suspensién e interrupcién de la\n",
            "prescripcion, en este caso de la acci6én. \n",
            "Sumado a ello, debe considerarse ademds que atin cuando en la,\n",
            "presente causa fueron dictadas la .D: N 22 de fecha 25 de \n",
            "octubre de 1999 por el Juzgado de Primera Instancia en lo \n",
            "Criminal del 9 Turno que condenéd al recurrente HANS FBEEDRICA \n",
            "SCHUCHARDT MAYER a la pena privativa de libertad de ocho afios, y\n",
            "el Acuerdo y Sentencia N 136 de fecha 13 de diciembre del 2004! \n",
            "dictada por el Tribunal de Apelacion en lo Criminal 1 Sala que?\n",
            "confirmo la condena impuesta al recurrente, ello no es 6ébice para\n",
            "que, en el caso de que el plazo haya expirado se disponga 1a!\n",
            "prescripcioén de la accidén penal, puesto que para que seay\n",
            "ejecutable y tenga ese caracter de norma juridica particular \n",
            "apuntando anteriormente, debe revestir 1 8 calidad de cosa\n",
            "JUZGAdA. mor nnn nnn rrr rrr\n",
            "Ciertamente, la sentencia definitiva pone fin al proceso y!\n",
            "es el objeto ultimo del ejercicio de la accién penal: que unl\n",
            "érgano jurisdiccional declare con razén la reclamacién hecha o la\n",
            "defensa articulada y en consecuencia obligue, en otras palabras \n",
            "debe estar fi der\n",
            "poe\n",
            "ejecutable,\n",
            "naldo Levera\n",
            "Secretario\n",
            "eee \n",
            "Si la sentencia no esta firme, no puede aseverarse que la\n",
            "cuestién haya sido resuelta definitivamente, lo que implica\n",
            "ademas que el plazo sigue corriendo, puesto que si bien ha sido\n",
            "ejercida la accién la misma atin no ha tenido la definicion\n",
            "esperada, ejecutable y con calidad de certera, norma juridica\n",
            "particular (aplicable exclusivamente a las partes) . \n",
            "Ls Ese es el criterio constante de esta Sala Penal, expuesto en\n",
            "los fallos citados precedentemente y muy especialmente en el\n",
            "Acuerdo y Sentencia N 1474 de fecha 11 de diciembre del 2006 en\n",
            "RECURSO ESTRAORDINARIO DE CASACION INTERPUESTO POR EL Abog.\n",
            "CARLOS BRITEZ CARDENAS en los autos: ENRIQUE GARCIA Y JOSE\n",
            "LEOPOLDO MENDOZA S H.P. C LA VIDA HOMICIDIO CULPOSO que sefiala\n",
            "por voto unanime: ..La resolucién definitiva a la que hace\n",
            "referencia la ley es aquella que tiene la virtualidad de la COSA\n",
            "JUZGADA MATERIAL, de lo contrario si solo bastara con una\n",
            " Sentencia definitiva no notificada, en otras palabras que no\n",
            "esta firme, la misma no se puede ejecutar, no hace cosa juzgada y\n",
            "en consecuencia no pone fina al procedimiento. Con lo cual,\n",
            "ademés se podria extender el proceso en el tiempo violando el\n",
            "principio de tiempo razonable. En otras palabras, debe quedar\n",
            "establecido que cuando la ley refiere a sentencia definitiva en\n",
            "el art. 136 trata de aquella que esta firme y ejecutoriada,\n",
            "contra la que no procede recurso, salvo claro el caso del recurso\n",
            "de revisién, de finalidad especifica y limitada. ,. \n",
            "Igualmente la doctrina sefiala al respecto: ..Ahora bien,\n",
            "independientemente de ello y de la oportunidad procesal en que se\n",
            "verifique la prescripcién, dijimos que por ser la prescripcioén\n",
            "pénal instituto de derecho material una cuestidén de orden publico\n",
            "que por ende opera de pleno derecho, debe ser declarada incluso\n",
            "de oficio en cualquier estado y grado de la causa, atin durante\n",
            "el trdmite recursivo si dan las situaciones que la ley prevé \n",
            "( la prescripcién en el proceso penal por Maximiliano Hairabedian\n",
            "y Federico Zurueta; editorial Mediterranea, p. 60,\n",
            "Cérdoba Repiblica Argentina).\n",
            "11\n",
            "1\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE \n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "ES lp, YEGROS Y OTROS S DEFRAUDACION,\n",
            "Sey Zhe,\n",
            "Cane Suprema deSadiicia FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "Vy Pe ry OTROS\" \n",
            "rca \n",
            "roe a\n",
            "ar. oe . \n",
            " 3 noe Afr ar que no se puede declarar la prescripcién de la\n",
            "Oe Ex 2 . . .\n",
            "eaecions én razén a que ha sido dictada una sentencia que no esta\n",
            " ae .\n",
            "firmé; que no tiene calidad de cosa juzgada y por ende no\n",
            "fis tt\n",
            "énstituye norma juridica particular vinculante para las partes,\n",
            "implica que todos ellos sometidos a proceso son culpables, y que\n",
            "ee\n",
            "el proceso se dirige tnica y exclusivamente a la atribucidén de\n",
            "responsabilidades concretas, que ya en abstracto se encuentran !\n",
            "previstas, pudiendo el érgano jurisdiccional tomarse todo el'\n",
            "tiempo que requiera para condenar. Con tales premisas, se deberia!\n",
            "eliminar el tramite del proceso penal y directamente resolver \n",
            "sentencias condenatorias para todos los casos, lo cual resulta un,\n",
            "absurdo juridico. La Comisién Interamericana de Derechos Humanos,\n",
            "ha expresado: que el Estado proceda al enjuiciamiento penal de ip\n",
            "todos los delitos, no justifica que se dedique un periodo de\n",
            "tiempo ilimitado a la resolucidén de un asunto de indole criminal.\n",
            "De otro modo, se asumiria de manera implicita que el Estado \n",
            "siempre enjuicia a culpables y que, por lo tanto, es irrelevante,\n",
            "el tiempo que se utilice para probar la culpabilidad (Eduardo! \n",
            "I\n",
            "Jauchen - Derechos del Imputado, p 322, ed. Rubinzal Culzoni,. \n",
            "Santa Fé Argentina, 2005) . \n",
            "En el mismo sentido se ha dicho: ..la sola presencia de una\n",
            "causal extintiva de accién debe ser estimada independientemente\n",
            "cualquiera sea la oportunidad de su produccién y de su \n",
            "conocimiento por el tribunal, toda vez que en términos\n",
            "Procesales significa un impedimento para continuar ejerciendo\n",
            "los poderes de accién y jurisdiccién en procura de wun\n",
            "Pronunciamiento sobre el fondo. Es decir, que no queda librada a;\n",
            "la voluntad del juzgador la posibilidad de optar por realizar un)\n",
            "andlisis objetivo o subjetivo de causales, sino que la ley imponel\n",
            "un camino a y mo corresponde que el Tribunal se!\n",
            "Pronuncie sobre ntenido de. ecurso de casacion incoado; y;\n",
            "a t\n",
            "uLFOSLANCO \n",
            "Ministro\n",
            "Ph\n",
            "Abog. Arnaldo Levera\n",
            "Secretario\n",
            " \n",
            " \n",
            "12\n",
            "fondo de la cuestién, deben asegurarse de que el hecho no esté\n",
            "cubierto por la prescripcién (Maximiliano Hairabedian y Federico\n",
            "Zurueta, ob. Cit p. 53). En otras palabras: previo a cualquier\n",
            "discusién sobre lo sustancial se deben verificar los plazos\n",
            "procesales y en caso necesario, disponer la prescripcién de la\n",
            "accién, independientemente a las presiones de las partes y al\n",
            "momento procesal, sin necesidad de un debate sobre los hechos,\n",
            "sus probanzas y la calificacion juridica que ameriten, puesto que\n",
            "el instituto tiene caracter eminentemente cuantitativo \n",
            "constatacién de los plazos procesales y no cualitativo, \n",
            "Dentro de este contexto, debe en primer término determinarse\n",
            "cual ha sido la calificacién conferida a los hechos investigados\n",
            "en la presente causa. En este sentido, atin cuando la misma tuvo\n",
            "inicio bajo la vigencia del Cédigo Penal de 1910, el Juez de\n",
            "Primera Instancia, al momento de calificar las conductas de los\n",
            "procesados, entendiéd, que por ser de contenido y alcances mas\n",
            "benignos, en directa aplicacién de lo dispuesto por el art. 14 de\n",
            "14 C.N., correspondia la aplicacién de la nueva ley penal,\n",
            "calificando la conducta del peticionante en lo dispuesto por el\n",
            "art. 192 inc. 2 . Del nuevo C.P. en concordancia con el art. 29\n",
            "del mismo cuerpo legal. En igual sentido, el tribunal de\n",
            "Apelacién Penal dispuso que la calificacién debia ser la\n",
            "dispuesta por el art. 192 inc. 2 del C.P. en concordancia con lo\n",
            "dispuesto por los articulos 246 y 29 del mismo cuerpo legal. \n",
            "En efecto, por S.D. N 22 de fecha 25 de octubre de 1999 por\n",
            "el Juzgado de Primera Instancia en lo criminal del 9 Turno: \n",
            "1..2..3 CALIFICAR el hecho antijuridico atribuido a la conducta de\n",
            "(..) HANS FRIEDRICH SCHUCHARDT MAYER dentro de las prescripciones\n",
            "de los articulos 29 y 192 inciso 2 del Nuevo Cédigo Penal... 4...5..6\n",
            "CONDENAR A (..) 6.4 HANS FRIEDRICH SCHUCHARDT MAYER a la PENA\n",
            "PRIVATIVA DE LIBERTAD DE OCHO (8) ANOS . Por su parte, el acuerdo\n",
            "y Sentencia N 136 de fecha 13 de diciembre del 2004 dictada por\n",
            "el tribunal de Apelacién en lo criminal, 1 Sala determind:\n",
            " DESESTIMAR el recurso de nulidad; MODIFICAR la sentencia\n",
            "recurrida, S.D. N 22 de fecha 25 de octubre de 1999, dictada por\n",
            "2 . .\n",
            "el Juez en lo criminal del Noveno Turno, y en consecuencia;\n",
            "13\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO ve!\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN \n",
            "YEGROS Y OTROS S DEFRAUDACION, 1\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS \n",
            "OTROS nnn nnn nnn nnn nnn nn nnn t nn sr nnn\n",
            "FRIEDRICH SCHUCHARDT dentro de las disposiciones de los articulos\n",
            "wie\n",
            " 29 incisos 1 y 2 y el articulo 70 todos del Cédigo penal (..)\n",
            "DEJAR incélume los demas puntos de la sentencia recurrida . -\n",
            "En prosecucién del andlisis de la procedencia o no de la\n",
            "declaracién de prescripcién de la accién penal en la presente \n",
            "causa, es menester traer a colacién lo dispuesto por las normas\n",
            "penales que determinan las conductas tipicas por las que fue \n",
            "condenado HANS FRIEDRICH SCHUCHARDT lo dispuesto por el art. 1924\n",
            "inc. 1 del C.P. en concordancia con el art. 246 del mismo cuerpo\n",
            "El art. 192 del C.P. determina: \"..LESION DE CONFIANZA 1 El\n",
            "que en base a una ley, a una resolucién administrativa o a un \n",
            "contrato, haya asumido la responsabilidad de proteger un interés,\n",
            "patrimonial relevante para un tercero y causara o no evitara,!\n",
            "dentro del dmbito de proteccidén que le fue confiado, un perjuicio\n",
            "patrimonial, serd castigado, con pena privativa de libertad de\n",
            "hasta cinco afios o con multa 2 . En los casos especialmente\n",
            "graves la pena privativa de libertad podrd ser aumentada hasta\n",
            "dieZ anos...\" . mr r e\n",
            "Por su parte, el art. 246 del C.P. dispone: PRODUCCION DE;\n",
            "DOCUMENTOS NO AUTENTICOS. 1 . El que produjera o usara un\n",
            "documento no autentico con intencidén de inducir en las relaciones\n",
            "juridicas al error sobre su autenticidad, serd castigado con penal\n",
            "privativa de libertad de hasta cinco afios o multa. 2 Se\n",
            "entenderd como: 1. Documento, la declaracién de una idea\n",
            "formulada por una persona de forma tal que, materializada,\n",
            "i\n",
            "2. No autentico, un \n",
            "Abog. Arnaldo Levera\n",
            "Secretario\n",
            "102 inc. 3.\n",
            "14\n",
            "casos especialmente graves, la pena privativa de libertad podrd\n",
            "ser aumentada hasta diez .afios .-\n",
            "Igualmente, deben considerarse las normas atinentes a la\n",
            "prescripcién de la accién penal, presentes en el cédigo Penal. En\n",
            "primer término, lo dispuesto por el art. 102 del C.P. Plazos 1 .\n",
            "Los hechos punibles prescriben en 1. Quince afios, cuando el\n",
            "limite méximo del marco penal previsto se de quince afios o mas de\n",
            "pena privativa de libertad; 2. Tres afios cuando el limite maximo\n",
            "del marco penal previsto sea pena privativa de libertad de hasta\n",
            "tres afios o pena de multa; 3. En un tiempo igual al maximo de la\n",
            "pena privativa de libertad en los demas casos 2 El plazo correrd\n",
            "desde el momento en que termine la conducta punible. En caso de\n",
            "ocurrir posteriormente un resultado que pertenezca al tipo legal,\n",
            "el plazo correrd desde ese momento. 3 . Son imprescriptibles los\n",
            "hechos punibles, previstos en el art. 5 de la Constitucién. \n",
            "Por ultimo, el art. 104 del C.P. sefiala: interrupcién. 1 .\n",
            "La prescripcién serd interrumpida por: 1. un auto de instruccién\n",
            "sumarial; 2. Una citacidén para indagatoria del inculpado; 3. Un\n",
            "auto de elevacién de la causa al estado plenario; 6. Un escrito\n",
            "de fiscal peticionando la investigacion; y 7. Una diligencia\n",
            "judicial para actos investigativos en el extranjero. 2 . Después\n",
            "de cada interrupci6n, la prescripcién correrd de nuevo. Sin\n",
            "embargo, operara la prescripcién, independientemente de las\n",
            "interrupciones, una vez trascurrido el doble del plazo de la\n",
            "prescripcion, n-n-\n",
            "Consecuentemente, a los fines de establecerse si una accién\n",
            "ha prescripto o no, debe estarse a la calificacién realizada por\n",
            "el juez y concordar ello con los plazos previstos en el art. 102\n",
            "del C.P., en concordancia con lo dispuesto por el art. 104 del\n",
            "mismo cuerpo legal. \n",
            "El inicio del cdémputo del plazo de prescripcién, no es\n",
            "coincidente con el plazo del proceso, en otras palabras, el\n",
            "conteo no inicia con la primera resolucién de la causa o con su\n",
            "notificacién al procesado, sino mucho antes, al concluir la\n",
            "ejecucién del Hecho Punible, tal como lo prescribe el citado art.\n",
            "Para determinar ello, debemos fijarnos en lo\n",
            "Gorte Suprema det usticia\n",
            "Beak CB\n",
            "15\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN \n",
            "YEGROS Y OTROS s DEFRAUDACION, \n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS\" nnn nner nnn rrr rrr nan\n",
            "pe\n",
            "wWrestablécido por las resoluciones definitivas dictadas en la\n",
            "yy\n",
            "oy quedado determinado que existi6 una adulteracién de la!\n",
            "i\n",
            "de la Antelco dirigida al Gerente de Operaciones ,\n",
            "Internacionales del Banco Central del Paraguay, N 321 de fecha \n",
            "22 de febrero de 1996, en la cual quedaba como beneficiario el \n",
            "Banco Central del Paraguay. Igualmente por Nota N 821 de fecha,\n",
            "22 de febrero de 1996 (supuesta adulteracién de la nota N 321)\n",
            "dirigida por la Antelco al Gerente de Operaciones del Banco;\n",
            "Central del Paraguay, en la cual quedaba como beneficiario el\n",
            "Banco de Desarrollo del Paraguay S.A., la que finalmente fue\n",
            "acreditada a la Cta. Cte. N 500086-1, por nota de crédito-\n",
            "DOPI DDP N 15 96 en fecha 29 de febrero del 1996. De lo cual se \n",
            "desprende, que las conductas que se consideraron lesivas al orden \n",
            "juridico se produjeron entre el 22 de febrero de 1996 y el 29 ae \n",
            "febrero del mismo afio, por lo que se considerardé esta ultima' \n",
            "fecha como punto de partida para el cémputo de la prescripcién. '\n",
            "Debe considerarse ademas, el art. 104, inc. 2, Ultima parte!\n",
            "que determina que aun con las interrupciones, si transcurriere el,\n",
            "doble del plazo de la prescripcién, la misma prescribe. En ely\n",
            "presente caso, tanto el tipo penal de Lesién de Confianza, como!\n",
            "el de Produccién de documentos no auténticos disponen una sancion\n",
            "punitiva de hasta cinco afios cuando se despliega la conducta,\n",
            "prevista en el tipo base (art. 14, inc. 1, numeral 3). En \n",
            "relacién a la Lesién de confianza, si bien es cierto se hal\n",
            "condenado una conducta agravada, ella no debe considerarse a los \n",
            "efectos del computo de la prescripcién, puesto que lo que\n",
            "'\n",
            "contiene el ine. del art. 192 es una agravacién de lal\n",
            "nuevos elementos, sino que agrava la penay\n",
            "la\n",
            "conducta. No introfi\n",
            "conducta\n",
            "fidera que\n",
            " ALIC\n",
            "AMET\n",
            "CADSR Ministre\n",
            "Ab naldo Levera\n",
            "Secretario\n",
            "fe\" \n",
            "eSent causa. orn enn rrr rrr nnn gna \n",
            " \n",
            "Y es que ese criterio que impera en la doctrina, que a los\n",
            "efectos del cdémputo de la prescripcién debe estarse a lo\n",
            "dispuesto por el tipo base y no al tipo legal. A mayor\n",
            "abundamiento, la doctrina apunta: Al regirse todos los términos\n",
            "de prescripcién de la accién por la pena establecida en el tipo\n",
            "Penal, éste adquiere superlativa y decisiva importancia.\n",
            "Entendemos que durante la instruccién hay que estarse al tipo\n",
            "penal seleccionado en primer lugar en el requerimiento de\n",
            "instruccién fiscal o en su defecto a la calificacién seleccionada\n",
            "para algin trdmite de libertad provisional. Si se ha dictado el\n",
            "auto de procedimiento el mismo determinara el encuadre penal del\n",
            "hecho, mientras que si el fiscal ya emitid la requisitoria de\n",
            "elevacién a juicio la misma delimitara la figura penal a\n",
            "aplicarse a los fines de la prescripcién de la accién. La duda en\n",
            "cuanto a la determinacién del tipo penal aplicable debe ser\n",
            "interpretada siempre a favor del imputado. De acuerdo a lo\n",
            "expuesto, no depende el plazo de prescripcién de la eleccién que\n",
            "haga el juez en el momento de dictar sentencia.. . (Jorge C.\n",
            "Baclini, Prescripcién Penal- Analisis doctrinal y\n",
            "jurisprudencial. Comentarios a la ley 25990, p. 61, ed. Juris,\n",
            "afio 2005, Rosario Argentina) . 92 - nnn nnn nnn nnn\n",
            " No ejerce entonces ninguna influencia ni puede, por ende,\n",
            "adquirir trascendencia o significacién en la prescripcién de la\n",
            "accién todo lo atinente a los atenuantes o agravantes \n",
            "(Sproviero, Juan H., Prescripcién de la Accién y de la Pena, p.\n",
            "130, Ed. Abaco, Buenos Aires-Argentina, afio 2001). La mencién\n",
            "doctrinaria hace referencia a un sistema penal en donde los\n",
            "condicionamientos de agravacién y atenuaciédn de la pena se\n",
            "encuentran descriptos en la parte general de la ley penal, de tal\n",
            "manera que a la hora de calificar el hecho punible, el Juez debe\n",
            "tomar como base el tipo base y luego adecuar la sancién de\n",
            "acuerdo a la gravedad o insignificancia, descriptos en la parte\n",
            "general. Tal sistema, ha sido desechado por la nueva ley penal,\n",
            "que prefiere, para cada tipo penal, individualizar los casos de\n",
            "agravacion o suavizacién de la pena merecida en razén a la\n",
            "conducta desplegada. La exposicién de motivos del Anteproyecto\n",
            "del Cédigo Penal lo explica de la siguiente manera: ..La\n",
            " \n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "YEGROS Y OTROS s DEFRAUDACION,\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS\" 933 n nnn nn nnn nnn nnn nnn nnn nnn\n",
            "17\n",
            "es ipo fei de la conducta punible parte del tipo base. Respecto\n",
            " Geo o 3 de comisién mas o menos graves por circunstancias\n",
            "di fonwles no ha dado buen resultado la técnica legislativa de\n",
            "colocar agravantes y atenuantes generales en la Parte General sin\n",
            "consideracién de aspectos axiolégicos. Por eso, el anteproyecto\n",
            "prefiere una individualizacién de los casos correspondientes en\n",
            "ala Parte Especial (Cédigo Penal de la reptblica del\n",
            "Paraguay Coleccién de Derecho Penal, division de Investigacién,\n",
            "Legislacién y Publicaciones, Corte Suprema de Justicia, p.\n",
            "LXXXII, Asuncién-Paraguay, afio 1999) . \n",
            "No obstante ello, como bien lo dice la citada resefa, el\n",
            "tipo base describe la conducta y la sancién aplicable, todo lo\n",
            "demas son circunstancias que pueden agravar o atenuar la sancién\n",
            "penal, por lo cual, a los fines del cémputo del plazo para la\n",
            "prescripcién, por una interpretacién mas favorable para el\n",
            "encausado (art. 5 del C.P.P.), corresponde determinar que dicho\n",
            "plazo se contabiliza en relacién con la sancién dispuesta para la\n",
            "conducta del tipo base, nnn nnn nn nnn nnn nnn nnn\n",
            "Yendo a la cuestiéri puntual, la causa en la relacién HANS\n",
            "FRIEDRICH SCHUCHARDT MAYER inicié para en virtud al A.I. N 332\n",
            "de fecha 25 de marzo de 1997, por la cual se le instruyé sumario.\n",
            "La calificacién final de su conducta fue incursa en lo dispuesto\n",
            "por el art. 192 del C.P. (Lesién de Confianza) 246 del mismo\n",
            "cuerpo legal, cuyo marco penal base es de 6 meses a cinco afios o\n",
            "multa. Habiendo sido condenado a la pena privativa de libertad de\n",
            "ocho afios, se considerara lo previsto por el art. 102 inc. 1,\n",
            "w\n",
            "numeral 3: en un tiempo igual al mdximo de la pena privativa\n",
            "de libertad en los demaés casos , 7- 337 n nnn rrr\n",
            "Computando desye el 29 de febrero de 1996 y considerando lo\n",
            "dispuesto pof drticulos 192 y 246 del C.P. en concordancia\n",
            "Ministro,\n",
            "Abog Arnaldo Levera\n",
            "Secretario\n",
            "ae Ae\n",
            "en\n",
            "como se ha dicho, se constata conforme al art. 104 inc. 2, in\n",
            "fine que ha trascurrido el doble del plazo de la prescripcién,\n",
            "ya que desde el 29 de febrero de 1996 hasta el 01 de marzo del\n",
            "2006 han transcurrido diez afios desde que se desplego la conducta\n",
            "punible, con lo cual se concluye que en la presente causa ya ha\n",
            "operado la prescripcién de la accién penal, por imperio de lo\n",
            "dispuesto por el art. 104 inc. 2 del C.P.P. \n",
            "En ese orden de cosas, es menester ademas aclarar el derrote\n",
            "de la causa en la presente instancia, de acuerdo a las\n",
            "éonstancias de autos y a los registros obrantes en los cuadernos\n",
            "respectivos. En ese contexto la causa fue remitida por primera\n",
            "vez a este despacho en fecha 21 de marzo de 2006. En fecha 20 de\n",
            "julio del 2007, el procesado HANS FRIEDRICH SCHUCHARDT MAYER\n",
            "solicita la prescripcién de la accion penal, por lo que el 16 de\n",
            "agosto de 2007 los autos fueron remitidos a la Secretaria\n",
            "Judicial III a los fines de la tramitacién del pedido. En fecha\n",
            "12 de diciembre del 2007 son remitidos nuevamente los autos a\n",
            "éste preopinante, volviendo a remitirse los autos a la Secretaria\n",
            "Judicial III en fecha 13 de mayo de 2008 a los fines del\n",
            "reemplazo del Prof. Dr. Wildo Rienzi Galeano y la integracién por\n",
            "otro Ministro de la Corte Suprema de Justicia. Por Ultimo, en\n",
            "fecha 18 de julio de 2008 fueron remitidos nuevamente los autos a\n",
            "los fines de la emisién del voto. Como se puede notar, al momento\n",
            "de remitirse por primera vez la causa a los fines del estudio del\n",
            "caso 22 de marzo del 2006 la causa ya habia prescripto se\n",
            "produjo el 1 de marzo del 2006 , por lo que no existe otra\n",
            "alternativa que la de fallar en consecuencia.- \n",
            "Corresponde por tanto se decrete que en la presente causa se\n",
            "produjo la prescripcién de la accién penal en relacidén al\n",
            "ciudadano HANS FRIEDRICH SCHUCHARDT MAYER el 1 de marzo de 2006\n",
            "en razon a haber transcurrido diez afios desde que se produjeron\n",
            "las conductas penales atribuidas al mismo, de conformidad a lo\n",
            "dispuesto por los articulos 192, 246, 101, 102, 104 del C.P.. ES\n",
            "MI VOTO, 7-3 nn nr nnn nnn nnn rrr nnn nnn neers snr\n",
            "- 19 j\n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE \n",
            "CASACION INTERPUESTO POR EL SR. HANS \n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN!\n",
            "YEGROS Y OTROS s DEFRAUDACION, \n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y \n",
            "OTROS : ;\n",
            "ALS \" o\n",
            "3 si 88 sieturno, la Ministra PUCHETA DE CORREA dijo: Disiento con \n",
            "que antecede del ilustre Ministro preopinante. -\n",
            " A ag sa\n",
            "antes de estudiar la admisién del recurso de casaci6n\n",
            "rado, es menester analizar la prescripcién de la accién\n",
            "penal, por ser ésta de previo pronunciamiento por su caracter de\n",
            "El andlisis de esta cuestién es sencillo en el presente caso\n",
            "y no requiere de mucho examen. Los hechos fdcticos de este juicio\n",
            "tienen su inicio en el afio 1996, siendo la causa establecida para\n",
            "muchos imputados y describiéndose la conducta de cada uno de\n",
            "ellos, entrando asi el encausado Hans Schuchardt. Este juicio\n",
            "fue sancionado a tenor de las disposiciones de la Ley 1160 97 y\n",
            "no puede hallarse resultados posteriores a la conducta del\n",
            "penal debe correr desde el afio 1996. \n",
            "i\n",
            "r\n",
            "t\n",
            "'\n",
            " \n",
            "1\n",
            "ij\n",
            " \n",
            "I\n",
            "acusado, por tanto, el plazo de la prescripcién de la accién\n",
            "Para el recurrente, fue dictada la SD N 22 de fecha 25 de \n",
            "Octubre de 1999, en donde, a lo pertinente del andlisis de esta \n",
            "prescripcién, su conducta fue calificada dentro de lo que dispone \n",
            "el articulo 192 del CP, que establece el plazo de cinco afios como\n",
            "f\n",
            "pena privativa de libertad, debiendo ser utilizado este plazo a ;\n",
            "los efectos de la prescripcién, por ser el tipo base de dicha \n",
            "El articulo 102 inciso 1 del CP expresa: Los hechos \n",
            "punibles prescriben en: 1 quince afios, cuando el limite maximo\n",
            "privativa de libertad; 2 tres afios, cuando el limite maximo del\n",
            "marco penal prevj sea pena privativa de libertad de hasta tres\n",
            "del marco penal previsto sea de quince afios o mas de pena \n",
            " \n",
            "afios o pena de \n",
            "'\n",
            "privativa de 1\n",
            "Ministro\n",
            "Abag. aldo Levera \n",
            "Secretaria \n",
            "20\n",
            " En este momento, se debe efectuar la subsuncién de la\n",
            "conducta calificada dentro de las propuestas hechas en el\n",
            "articulo 102 del mismo cuerpo legal. Al respecto, la misma debe\n",
            "ser incursada dentro de lo que indica el numeral 3 del inciso 1 \n",
            "del articulado citado. Esto se hara asi por no poderse\n",
            "contemplar sus agravantes o atenuantes, 9 5-5-5 \n",
            "Asi dado este razonamiento, esta accidén deberia prescribir a\n",
            "los cinco afios del afio determinado, contando desde el afio 1996\n",
            "por lo que deberia prescribir en el afio 2001; no es necesario\n",
            "analizar en este caso las interrupciones acaecidas. \n",
            ". Sin embargo, como se manifest6 mas arriba, en este juicio\n",
            "para el hoy recurrente, se ha dictado una sentencia en el afo\n",
            "1999, siendo la misma dictada en tiempo vigente, antes del afio\n",
            "2001, y que impide que la prescripcién sea valorada a posterior\n",
            "de ella. errr nn rn rrr rrr nnn nnn nn\n",
            "Esto es asi porque cuando un individuo sufre un injusto, el\n",
            "Estado le otorga para su reparacién una accién penal, y el\n",
            "objetivo de esta accién es llevar al causante del injusto ante un\n",
            "tribunal que juzgue su conducta, pudiendo dicho tribunal absolver\n",
            "o condenar, lo cual a los efectos prescriptivos no tiene valor\n",
            "alguno. \n",
            "Pero producido ese juicio, la accién no tiene mas sentido\n",
            "Aalguno, desaparece, la accién penal ya cumpli6é su objetivo, ya\n",
            "cumpliéd su sentido de existir, cual era el llevar al agraviante\n",
            "ante el tribunal, y cumplido esto, la misma como se reitera,\n",
            "desaparece, no existe mas, por ello, no se puede prescribir algo\n",
            "que ya no existe mas. Esta es la explicacién del por qué no\n",
            "figura la sentencia como causal de interrupcién, porque a partir\n",
            "de ella no hay mas nada que se pueda interrumpir. \n",
            "Siguiendo con el razonamiento delineado, es reconocido ya el\n",
            "criterio asentado en esta Corte Suprema de Justicia consistente\n",
            "én que no es posible debatir la prescripcién de la accién si es\n",
            "que se ha dictado la sentencia dentro del proceso especifico, es\n",
            "decir, si dentro del plazo de vigencia de la accién, ha recaido\n",
            "una sentencia, entonces no se puede hablar de prescripcidén de la\n",
            "accién penal, y por ende, si la sentencia ha recaido con\n",
            "posterioridad al lapso de tiempo en el cual la accion se hallaba\n",
            "Abo naldo Levera\n",
            " \n",
            " \n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE!\n",
            "CASACION INTERPUESTO POR EL SR. HANS..\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN \n",
            "YEGROS Y OTROS s DEFRAUDACION, '\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS\" e enn nnn nnn nr nnn \n",
            "21\n",
            "te Suprema de Justicia en varios fallos, citando como ejemplos\n",
            "las causas caratuladas: Valeria Ortiz de Esteche s Lesién de \n",
            "enfianza; Julio César Valdéz s Lesién; Maria Teresa Lopez\n",
            "Moreira y otra s Calumnia; Oscar Luis Bernal s Lesién Culposa \n",
            "y Mario Salinas s Peculado, Rodrigo Luis Ugarriza s Lesion \n",
            "Culposa y Tomas Cantero Aguilera s Lesién Culposa, entre otros. \n",
            "Esta jurisprudencia deriva de la excelente doctrina arrimada al\n",
            "respecto por el jurista aleman Jescheck. \n",
            "Este tratadista del Derecho penal indica, a mayor\n",
            "redundancia, que: Cuando ha sido dictada sentencia en primera\n",
            "instancia ya no puede tener lugar la prescripcién de la\n",
            "persecucion; los retrasos en el procedimiento de recursos\n",
            "quedan, pues, sin consecuencias en orden a la prescripcién .\n",
            "(Hans Jescheck y Thomas Weigend, Tratado de Derecho Penal Parte\n",
            "General, pagina 985) , 9-3-3 nnn nnn nnn nnn nnn nnn nnn nnn\n",
            "Siendo por ello que no importa a la prescripcién de la accién\n",
            "penal el hecho que la sentencia este firme o no, por todos los\n",
            "El antecedente que obra en el voto preopinante, A y S N \n",
            "1474 de fecha 11 de Diciembre de 2006, no es aplicable a esta\n",
            "causa, porque el mismo se refiere a la Extincidén de la accién,\n",
            " \n",
            " \n",
            "'\n",
            "!\n",
            "'\n",
            " \n",
            "i\n",
            " \n",
            "motivos arriba indicados, nnn nnn nnn \n",
            "iS\n",
            "i\n",
            " \n",
            "figura regulada en el articulo 136 y modificaciones del CPP, que\n",
            "nada tiene en similitud con la Prescripcién de la accion penal,\n",
            "regulada en el articulo 102 en adelante del CP. Los otros\n",
            "i\n",
            "pero se refieren a materias conexas\n",
            "Ministro\n",
            "Secretario\n",
            "22\n",
            "Por todo esto, no procede la prescripcién intentada y no es\n",
            "posible debatirla mas. 3-3 errr nnn\n",
            "En cuanto al recurso de casacién, sobre su fondo, se ve que\n",
            "el mismo no puede ser admitido a estudio, por no cumplir los\n",
            "requisitos del articulo 477 del CPP, ademas de otros. \n",
            "El escrito, obrante a fojas 111 en adelante, alega el\n",
            "numeral 3 del articulo 478 para fundar su casacién, pero en\n",
            "primer lugar no indica absolutamente ningin agravio dirigido\n",
            "contra el fallo del Tribunal de Apelacion, con lo que como se\n",
            "expres6, el articulo 477 del CPP queda incumplido. En las\n",
            "primera fojas, el escrito de casacién hace un relato de todas las\n",
            "acciones y hechos relevantes en la causa, para que a partir de la\n",
            "foja 118 de autos pase a analizar pormenorizadamente las pruebas\n",
            "rendidas en juicio, para terminar, a fojas 125 hasta el final, en\n",
            "cuestionar el fallo de primera instancia, el cual no puede ser\n",
            "objeto de casacién, siendo por ende inadmisible el recurso\n",
            "intentado, 9 nnn nnn nn nnn nen nn nnn nnn nn\n",
            "Asi las cuestiones resueltas, corresponde rechazar el pedido\n",
            "de prescripcién intentado y declarar inadmisible la casacién\n",
            "presentada, quedando confirmado asi el Acuerdo y Sentencia N 136\n",
            "de fecha 13 de Diciembre de 2004. Es mi voto.\n",
            "A su turno, el ministro TORRES KIRMSER manifiesta que se\n",
            "adhiere al voto de la Ministra PUCHETA DE CORREA, \n",
            "a\n",
            "Con lo que se dio por terminado eX acto firmando S.S.E E,\n",
            "todo por ante mi de\n",
            "ie\n",
            " \n",
            "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
            "CASACION INTERPUESTO POR EL SR. HANS\n",
            "FRIEDICH SCHUCHARDT en la causa: IVAN\n",
            "YEGROS Y OTROS s DEFRAUDACION,\n",
            "FALSIFICACION DE INSTRUMENTOS PUBLICOS Y\n",
            "OTROS : \n",
            "23\n",
            "Asuncion, Ad de agosto de 8088. -\n",
            "OS: Los méritos del Acuerdo que antecede, la-\n",
            "CORTE SUPREMA DE JUSTICIA SALA PENAL\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df['text'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.392814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  5000.000000\n",
              "mean      0.190600\n",
              "std       0.392814\n",
              "min       0.000000\n",
              "25%       0.000000\n",
              "50%       0.000000\n",
              "75%       0.000000\n",
              "max       1.000000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXNHCTPGiBr"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, temp_texts, y_train, y_temp = train_test_split(\n",
        "    df['text'], df['label'],\n",
        "    test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, y_val, y_test = train_test_split(\n",
        "    temp_texts, y_temp,\n",
        "    test_size=0.5, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 3500\n",
            "Validation samples: 750\n",
            "Test samples: 750\n",
            "\n",
            "label\n",
            "0    2843\n",
            "1     657\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print('Train samples:', train_texts.shape[0])\n",
        "print('Validation samples:', val_texts.shape[0])\n",
        "print('Test samples:', test_texts.shape[0])\n",
        "print()\n",
        "\n",
        "# print labels distribution in train\n",
        "print(y_train.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting train, val and test texts to csv...\n"
          ]
        }
      ],
      "source": [
        "print(\"Converting train, val and test texts to csv...\")\n",
        "train_texts.to_csv('corpus/train_texts.csv', index=False, header=False)\n",
        "val_texts.to_csv('corpus/val_texts.csv', index=False, header=False)\n",
        "test_texts.to_csv('corpus/test_texts.csv', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 15:48:36.541000: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-25 15:48:36.554567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-25 15:48:36.570718: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-25 15:48:36.575623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-25 15:48:36.588153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-25 15:48:37.470097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "Number of GPUs: 2\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "------------------------------------\n",
            "Model: Narrativa/legal-longformer-base-4096-spanish\n",
            "------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leon/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample train input_ids: [101, 4654, 5669, 25099, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1050, 5890, 1012, 5890, 1012, 6021, 1012, 5890, 1012, 2268, 1012, 23475, 1000, 1012, 2019, 2080, 2249, 1010, 1050, 5354, 2620, 1010, 1042, 29401, 6421, 1058, 23223, 1012, 9353, 13094, 3527, 1061, 2741, 27742, 16371, 5017, 2080, 1024, 21864, 8034, 13663, 2015, 7367, 5054, 2696, 1061, 16371, 18697, 4372, 2474, 20759, 2139, 2004, 4609, 10446, 1010, 3007, 2139, 2474, 3072, 2050, 3972, 13884, 1010, 1037, 3050, 3280, 2480, 1061, 16371, 18697, 22939, 2015, 1010, 3972, 2033, 2015, 2139, 12022, 3695, 3972, 2019, 2080, 9998, 23689, 9986, 2063, 9765, 28574, 2128, 19496, 12269, 4372, 2474, 16183, 2050, 2139, 9353, 13094, 12269, 3050, 12411, 16610, 7163, 3367, 7352, 2139, 2474, 24970, 4765, 17417, 2863, 2522, 19731, 10514, 28139, 2863, 2139, 2074, 24108, 1010, 15935, 3786, 21885, 16405, 20318, 2050, 2139, 2522, 14343, 2050, 1010, 8254, 8566, 10270, 2080, 20036, 1061, 6446, 3814, 3841, 4221, 2480, 15544, 6906, 1010, 14405, 2063, 2771, 2474, 3595, 10980, 8285, 21885, 12956, 1010, 7367, 19817, 13006, 2080, 3449, 4654, 5669, 25099, 14418, 28970, 3527, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1010, 24528, 3449, 9353, 13094, 3527, 1061, 2741, 27742, 1050, 5179, 2139, 10768, 7507, 2654, 2139, 13323, 12083, 2890, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 23957, 2721, 10446, 4372, 8840, 18476, 1010, 14837, 16183, 2050, 2139, 2474, 3007, 1012, 3653, 25500, 9765, 21041, 2080, 2139, 3050, 14405, 26005, 16454, 2229, 3972, 25222, 2080, 1010, 2474, 24970, 4765, 17417, 2863, 2522, 19731, 10514, 28139, 2863, 2139, 2074, 24108, 1010, 24501, 4747, 25500, 3269, 14644, 5869, 9033, 25698, 26933, 1024, 1039, 1057, 1041, 1055, 1056, 1045, 1051, 1050, 1041, 1055, 1024, 9686, 4748, 15630, 7028, 3449, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 1029, 1012, 1011, 4372, 10514, 25222, 2080, 1010, 2765, 2050, 4013, 22119, 10111, 1029, 1012, 1011, 1037, 3050, 1041, 25969, 2891, 2139, 28283, 22311, 2099, 4895, 2030, 4181, 11498, 2474, 16258, 19570, 3258, 2139, 5869, 5448, 2229, 1010, 7367, 2613, 10993, 2080, 4895, 4066, 8780, 10861, 12098, 3217, 5558, 3449, 9033, 25698, 15781, 2765, 9365, 1024, 20036, 1010, 16405, 20318, 2050, 2139, 2522, 14343, 2050, 1061, 3841, 4221, 2480, 15544, 6906, 1012, 1011, 1037, 2474, 14837, 23391, 3508, 3269, 13775, 2050, 1010, 3449, 7163, 3367, 3217, 20036, 4487, 5558, 1024, 10861, 18499, 9353, 13094, 3527, 1061, 2741, 27742, 1050, 5179, 2139, 10768, 7507, 2654, 2139, 13323, 12083, 2890, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 23957, 2721, 10446, 4372, 8840, 18476, 1010, 14837, 16183, 2050, 2139, 2474, 3007, 1010, 18499, 2474, 12731, 2389, 7367, 24501, 4747, 25500, 4372, 7913, 27178, 8180, 2522, 20939, 1024, 1000, 1012, 1012, 1012, 1015, 1007, 1012, 1012, 1012, 1025, 1016, 1007, 1012, 1012, 1012, 1025, 1017, 1007, 12210, 2906, 2474, 2741, 27742, 23957, 27266, 2050, 1010, 1055, 1012, 1040, 1012, 1050, 2656, 1010, 3972, 2539, 2139, 14415, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 4895, 15457, 17753, 2389, 2139, 2741, 27742, 20014, 13910, 12173, 2080, 18499, 3449, 18414, 9351, 7762, 12943, 10179, 14343, 8473, 3022, 18609, 2343, 2063, 1012, 4372, 9686, 2063, 2741, 13820, 2474, 24501, 4747, 14194, 3258, 28667, 3126, 14615, 2050, 1055, 1012, 1040, 1012, 1050, 2656, 1010, 3972, 2539, 2139, 14415, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 4895, 15457, 17753, 2389, 2139, 2741, 27742, 20014, 13910, 12173, 2080, 18499, 3449, 18414, 9351, 7762, 12943, 10179, 14343, 8473, 3022, 18609, 2343, 2063, 1010, 24501, 4747, 25500, 1025, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 1016, 1007, 11703, 8017, 2906, 2053, 4013, 9024, 2050, 4372, 18414, 27113, 2474, 4839, 27742, 3972, 2002, 9905, 26136, 7028, 2139, 2572, 8189, 4143, 18499, 2112, 2063, 3972, 9353, 10383, 3527, 1037, 1012, 1041, 1012, 1025, 1017, 1007, 11703, 8017, 2906, 18609, 2053, 8285, 2099, 3972, 2002, 9905, 26136, 7028, 2139, 2572, 8189, 4143, 4372, 2566, 9103, 27113, 2139, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 1010, 2632, 9353, 10383, 3527, 1037, 1012, 1041, 1012, 18499, 3050, 4636, 24996, 2891, 4654, 14289, 4355, 2080, 4372, 3449, 4654, 8551, 3695, 2139, 2474, 2556, 2063, 24501, 4747, 14194, 3258, 1025, 1018, 1007, 14689, 4747, 6299, 2139, 12731, 14277, 2050, 1061, 19409, 18499, 6904, 24458, 2139, 7857, 2080, 1037, 1037, 1012, 1041, 1010, 1012, 1012, 1012, 1010, 18499, 3050, 4636, 24996, 2891, 4654, 14289, 4355, 2891, 4372, 3449, 4654, 8551, 3695, 2139, 2474, 2556, 2063, 24501, 4747, 14194, 3258, 1025, 1012, 1012, 1012, 1012, 1012, 1012, 1000, 1012, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 8670, 5558, 6986, 3217, 27085, 2080, 11268, 2229, 19301, 1010, 6970, 29513, 28667, 9236, 2080, 4469, 8551, 3981, 9488, 2139, 14124, 10446, 24528, 3449, 2991, 2080, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 2632, 4143, 2850, 1012, 3449, 28667, 29264, 2063, 9358, 2527, 10514, 2015, 29542, 25500, 2015, 19676, 28574, 10861, 2474, 24501, 4747, 14194, 3258, 29533, 3540, 2850, 9686, 23624, 14213, 15464, 15781, 1999, 11263, 8943, 2850, 1999, 6767, 9336, 3527, 3449, 2396, 1012, 4700, 2620, 4297, 19565, 1017, 3972, 1039, 1012, 1052, 1012, 1052, 1012, 4372, 7913, 27178, 8180, 23391, 3508, 2229, 3449, 28667, 29264, 2063, 23624, 14213, 2696, 1024, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 14405, 2229, 10861, 4487, 13102, 5643, 2099, 8840, 10861, 17254, 2063, 2019, 7934, 2474, 1055, 1012, 1040, 1012, 1050, 2656, 18499, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1010, 4372, 7352, 3540, 10514, 9353, 10383, 10446, 24528, 2474, 10861, 21835, 18678, 1010, 10250, 18513, 28574, 9765, 2050, 6204, 2050, 4372, 3449, 2741, 13820, 2139, 10861, 1025, 1000, 1012, 1012, 3449, 23957, 5802, 2618, 2004, 17897, 14477, 2552, 4183, 6784, 8915, 5017, 10980, 1041, 17727, 28121, 15781, 2632, 9353, 10383, 2099, 2632, 12152, 2139, 2741, 27742, 2139, 3653, 10755, 5555, 3406, 9530, 3050, 4636, 24996, 2891, 10861, 9811, 2063, 2074, 18513, 2906, 14477, 6232, 2050, 1037, 2474, 2741, 27742, 23957, 27266, 2050, 1012, 1012, 1012, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 3449, 13012, 8569, 2532, 16137, 7367, 20704, 24163, 1037, 1025, 1037, 1007, 8291, 1061, 2074, 18513, 2906, 2474, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1012, 1012, 1012, 1012, 1012, 1012, 1038, 1007, 13366, 9013, 3207, 2632, 12152, 2139, 2741, 27742, 1010, 1012, 1012, 1012, 1012, 1012, 1012, 1039, 1007, 2038, 2696, 7367, 2222, 20265, 2632, 8902, 5302, 1012, 1012, 1012, 1012, 1012, 1012, 1010, 1040, 1007, 2053, 27314, 2080, 9152, 3070, 9521, 10975, 5657, 3676, 2139, 2474, 10861, 21835, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 2053, 7367, 2613, 10993, 2080, 2474, 27314, 21736, 2139, 9353, 13094, 3527, 9530, 2474, 2624, 2050, 6232, 2050, 1025, 2474, 27314, 21736, 2139, 5869, 10975, 5657, 22083, 7367, 2613, 10993, 2080, 2139, 9353, 13094, 3527, 9530, 2474, 9530, 8159, 9013, 7405, 3972, 12152, 2139, 2741, 27742, 18499, 4226, 18609, 7367, 11487, 2080, 1010, 13075, 10735, 19960, 10735, 4013, 14479, 27416, 2015, 2053, 7367, 29454, 29206, 7405, 4948, 2566, 2080, 7367, 6449, 3771, 2239, 1061, 7367, 27314, 10464, 2078, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 2139, 9765, 2050, 2433, 2050, 2702, 6633, 2891, 10861, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 9686, 23624, 14213, 15464, 15781, 1999, 11263, 8943, 3527, 1061, 10861, 2053, 7367, 2613, 10993, 2080, 9530, 3366, 10841, 15781, 3672, 2063, 4895, 20302, 17417, 2015, 17540, 2890, 3050, 18975, 2891, 12731, 13728, 3981, 17340, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 10861, 3449, 6020, 9389, 3022, 7367, 3653, 10085, 6279, 2080, 4372, 9353, 8017, 2906, 14057, 13665, 2063, 28681, 2080, 9765, 2080, 1010, 19432, 4372, 19817, 3022, 26775, 12322, 4313, 18609, 9686, 3465, 25438, 2890, 4372, 28681, 3022, 5869, 24501, 4747, 14194, 3258, 2229, 8268, 2229, 1012, 1012, 1012, 24501, 12717, 10497, 2080, 17491, 6633, 2891, 19676, 2906, 12731, 21634, 9033, 9077, 1025, 1015, 1011, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 4487, 25572, 2850, 18499, 3449, 6020, 10482, 20692, 2015, 10514, 12693, 13247, 2229, 3972, 4013, 9623, 2080, 1012, 1012, 1012, 1025, 1016, 1011, 4839, 2063, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1010, 10861, 9765, 2050, 7279, 10265, 10111, 1061, 10861, 2053, 9146, 2063, 1010, 7367, 4487, 6593, 2063, 24501, 4747, 14194, 3258, 2632, 12734, 2050, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1017, 1011, 2474, 10514, 20528, 14693, 2239, 2139, 3729, 1051, 4487, 17729, 2618, 9686, 8050, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1018, 1011, 4839, 2368, 10975, 5657, 22083, 2053, 4078, 2923, 8524, 2015, 1010, 2053, 29454, 29206, 7405, 2015, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1019, 1011, 3449, 6020, 7857, 2080, 17540, 2890, 5869, 29454, 29206, 7405, 2015, 14916, 4305, 8883, 1010, 8840, 10861, 2053, 9765, 2050, 9146, 8524, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1020, 1011, 3449, 6020, 2632, 6449, 4313, 2474, 10514, 20528, 14693, 2239, 1061, 4487, 25572, 2099, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 1010, 15699, 3695, 3653, 10755, 5555, 3406, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1021, 1007, 28681, 2080, 7857, 2080, 11865, 2063, 24528, 2474, 10861, 21835, 1010, 23233, 2050, 1037, 5684, 1010, 9033, 6633, 28139, 4372, 24528, 1010, 8254, 6449, 4313, 2474, 10514, 20528, 14693, 2239, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1022, 1011, 4839, 2063, 4078, 9035, 10446, 3972, 4013, 11788, 27605, 4765, 2080, 1012, 1012, 1012, 1012, 1012, 1025, 1023, 1011, 28681, 2891, 3050, 14405, 26005, 16454, 2229, 3972, 2556, 2063, 4013, 9623, 2080, 1010, 9146, 2063, 28517, 28667, 9236, 2080, 4469, 8551, 3981, 9488, 2139, 14124, 10446, 1012, 1012, 1012, 1012, 1012, 1012, 1000, 1012, 4372, 3539, 2099, 2744, 5740, 9298, 2063, 4078, 2696, 10010, 10861, 3449, 24761, 18532, 2050, 28667, 9236, 20984, 21418, 3217, 3972, 5025, 24761, 18532, 2050, 4013, 9623, 2389, 7367, 19838, 2063, 4054, 3672, 2063, 18499, 3050, 26927, 12273, 11514, 10735, 2139, 23726, 29068, 27893, 1061, 2139, 17062, 2050, 8915, 2278, 12782, 1010, 9530, 8840, 12731, 2389, 3050, 2991, 2891, 2139, 10609, 14262, 17727, 15916, 25389, 2891, 4654, 20464, 2271, 11444, 3672, 2063, 18499, 3050, 19960, 10735, 1061, 5869, 2433, 3022, 17727, 15808, 10230, 18499, 3449, 19429, 14031, 8887, 1012, 4372, 28776, 8915, 28032, 4648, 1010, 17254, 2063, 1041, 25969, 6692, 2099, 4372, 3539, 2099, 11320, 6843, 3449, 9765, 21041, 2080, 2139, 2474, 4748, 15630, 12322, 18622, 14697, 3972, 28667, 9236, 2080, 4748, 14194, 13820, 1012, 3449, 20302, 17417, 2015, 2139, 2474, 4013, 22119, 22750, 7367, 1041, 25969, 6692, 2527, 15219, 3672, 2063, 3948, 1061, 9033, 3449, 28667, 9236, 2080, 5292, 15765, 2080, 6970, 14289, 4355, 2080, 1037, 1007, 4372, 2474, 2433, 2050, 1061, 2744, 5740, 3653, 11020, 28414, 2015, 18499, 2474, 20692, 1038, 1007, 9033, 2474, 24501, 4747, 14194, 3258, 17727, 15916, 25389, 2050, 4830, 11320, 6843, 1037, 3449, 1006, 23726, 29068, 27893, 16429, 15759, 11444, 1007, 1010, 1061, 1025, 1039, 1007, 9033, 11865, 2063, 2139, 8566, 6895, 3527, 18499, 21864, 2368, 5495, 2638, 6178, 6305, 27893, 11498, 3449, 4135, 1006, 23726, 29068, 27893, 4942, 15759, 11444, 1007, 1012, 4372, 12731, 21634, 2632, 3539, 2099, 18975, 2080, 2139, 20302, 17417, 2015, 2139, 2474, 4748, 15630, 12322, 18622, 14697, 1010, 6523, 13820, 1037, 2474, 2433, 2050, 2139, 1012, 1012, 1012, 1012, 1012, 1012, 4654, 5669, 25099, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1050, 5890, 1012, 5890, 1012, 6021, 1012, 5890, 1012, 2268, 1012, 23475, 1000, 1012, 2019, 2080, 2249, 1010, 1050, 5354, 2620, 1010, 1042, 29401, 6421, 1058, 23223, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 6970, 6873, 19570, 3258, 1024, 3449, 2396, 2594, 18845, 4805, 2620, 4372, 16557, 2319, 7405, 9530, 3449, 102]\n",
            "Type of classes: <class 'numpy.ndarray'>\n",
            "Classes: [0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Narrativa/legal-longformer-base-4096-spanish and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/leon/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using automodel\n",
            "RobertaForSequenceClassification(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Training arguments\n",
            "Batch size: 16\n",
            "Weight decay: 0.01\n",
            "Learning rate: 2e-05\n",
            "Warmup steps: 1000\n",
            "Metric for best model: f1\n",
            "Training the model...\n",
            "Verifying train dataset structure...\n",
            "{'input_ids': [101, 4654, 5669, 25099, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1050, 5890, 1012, 5890, 1012, 6021, 1012, 5890, 1012, 2268, 1012, 23475, 1000, 1012, 2019, 2080, 2249, 1010, 1050, 5354, 2620, 1010, 1042, 29401, 6421, 1058, 23223, 1012, 9353, 13094, 3527, 1061, 2741, 27742, 16371, 5017, 2080, 1024, 21864, 8034, 13663, 2015, 7367, 5054, 2696, 1061, 16371, 18697, 4372, 2474, 20759, 2139, 2004, 4609, 10446, 1010, 3007, 2139, 2474, 3072, 2050, 3972, 13884, 1010, 1037, 3050, 3280, 2480, 1061, 16371, 18697, 22939, 2015, 1010, 3972, 2033, 2015, 2139, 12022, 3695, 3972, 2019, 2080, 9998, 23689, 9986, 2063, 9765, 28574, 2128, 19496, 12269, 4372, 2474, 16183, 2050, 2139, 9353, 13094, 12269, 3050, 12411, 16610, 7163, 3367, 7352, 2139, 2474, 24970, 4765, 17417, 2863, 2522, 19731, 10514, 28139, 2863, 2139, 2074, 24108, 1010, 15935, 3786, 21885, 16405, 20318, 2050, 2139, 2522, 14343, 2050, 1010, 8254, 8566, 10270, 2080, 20036, 1061, 6446, 3814, 3841, 4221, 2480, 15544, 6906, 1010, 14405, 2063, 2771, 2474, 3595, 10980, 8285, 21885, 12956, 1010, 7367, 19817, 13006, 2080, 3449, 4654, 5669, 25099, 14418, 28970, 3527, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1010, 24528, 3449, 9353, 13094, 3527, 1061, 2741, 27742, 1050, 5179, 2139, 10768, 7507, 2654, 2139, 13323, 12083, 2890, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 23957, 2721, 10446, 4372, 8840, 18476, 1010, 14837, 16183, 2050, 2139, 2474, 3007, 1012, 3653, 25500, 9765, 21041, 2080, 2139, 3050, 14405, 26005, 16454, 2229, 3972, 25222, 2080, 1010, 2474, 24970, 4765, 17417, 2863, 2522, 19731, 10514, 28139, 2863, 2139, 2074, 24108, 1010, 24501, 4747, 25500, 3269, 14644, 5869, 9033, 25698, 26933, 1024, 1039, 1057, 1041, 1055, 1056, 1045, 1051, 1050, 1041, 1055, 1024, 9686, 4748, 15630, 7028, 3449, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 1029, 1012, 1011, 4372, 10514, 25222, 2080, 1010, 2765, 2050, 4013, 22119, 10111, 1029, 1012, 1011, 1037, 3050, 1041, 25969, 2891, 2139, 28283, 22311, 2099, 4895, 2030, 4181, 11498, 2474, 16258, 19570, 3258, 2139, 5869, 5448, 2229, 1010, 7367, 2613, 10993, 2080, 4895, 4066, 8780, 10861, 12098, 3217, 5558, 3449, 9033, 25698, 15781, 2765, 9365, 1024, 20036, 1010, 16405, 20318, 2050, 2139, 2522, 14343, 2050, 1061, 3841, 4221, 2480, 15544, 6906, 1012, 1011, 1037, 2474, 14837, 23391, 3508, 3269, 13775, 2050, 1010, 3449, 7163, 3367, 3217, 20036, 4487, 5558, 1024, 10861, 18499, 9353, 13094, 3527, 1061, 2741, 27742, 1050, 5179, 2139, 10768, 7507, 2654, 2139, 13323, 12083, 2890, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 23957, 2721, 10446, 4372, 8840, 18476, 1010, 14837, 16183, 2050, 2139, 2474, 3007, 1010, 18499, 2474, 12731, 2389, 7367, 24501, 4747, 25500, 4372, 7913, 27178, 8180, 2522, 20939, 1024, 1000, 1012, 1012, 1012, 1015, 1007, 1012, 1012, 1012, 1025, 1016, 1007, 1012, 1012, 1012, 1025, 1017, 1007, 12210, 2906, 2474, 2741, 27742, 23957, 27266, 2050, 1010, 1055, 1012, 1040, 1012, 1050, 2656, 1010, 3972, 2539, 2139, 14415, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 4895, 15457, 17753, 2389, 2139, 2741, 27742, 20014, 13910, 12173, 2080, 18499, 3449, 18414, 9351, 7762, 12943, 10179, 14343, 8473, 3022, 18609, 2343, 2063, 1012, 4372, 9686, 2063, 2741, 13820, 2474, 24501, 4747, 14194, 3258, 28667, 3126, 14615, 2050, 1055, 1012, 1040, 1012, 1050, 2656, 1010, 3972, 2539, 2139, 14415, 2139, 2249, 1010, 4487, 25572, 3527, 18499, 3449, 12152, 4895, 15457, 17753, 2389, 2139, 2741, 27742, 20014, 13910, 12173, 2080, 18499, 3449, 18414, 9351, 7762, 12943, 10179, 14343, 8473, 3022, 18609, 2343, 2063, 1010, 24501, 4747, 25500, 1025, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 1016, 1007, 11703, 8017, 2906, 2053, 4013, 9024, 2050, 4372, 18414, 27113, 2474, 4839, 27742, 3972, 2002, 9905, 26136, 7028, 2139, 2572, 8189, 4143, 18499, 2112, 2063, 3972, 9353, 10383, 3527, 1037, 1012, 1041, 1012, 1025, 1017, 1007, 11703, 8017, 2906, 18609, 2053, 8285, 2099, 3972, 2002, 9905, 26136, 7028, 2139, 2572, 8189, 4143, 4372, 2566, 9103, 27113, 2139, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 1010, 2632, 9353, 10383, 3527, 1037, 1012, 1041, 1012, 18499, 3050, 4636, 24996, 2891, 4654, 14289, 4355, 2080, 4372, 3449, 4654, 8551, 3695, 2139, 2474, 2556, 2063, 24501, 4747, 14194, 3258, 1025, 1018, 1007, 14689, 4747, 6299, 2139, 12731, 14277, 2050, 1061, 19409, 18499, 6904, 24458, 2139, 7857, 2080, 1037, 1037, 1012, 1041, 1010, 1012, 1012, 1012, 1010, 18499, 3050, 4636, 24996, 2891, 4654, 14289, 4355, 2891, 4372, 3449, 4654, 8551, 3695, 2139, 2474, 2556, 2063, 24501, 4747, 14194, 3258, 1025, 1012, 1012, 1012, 1012, 1012, 1012, 1000, 1012, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 8670, 5558, 6986, 3217, 27085, 2080, 11268, 2229, 19301, 1010, 6970, 29513, 28667, 9236, 2080, 4469, 8551, 3981, 9488, 2139, 14124, 10446, 24528, 3449, 2991, 2080, 4487, 25572, 3527, 18499, 3449, 12152, 2139, 2632, 4143, 2850, 1012, 3449, 28667, 29264, 2063, 9358, 2527, 10514, 2015, 29542, 25500, 2015, 19676, 28574, 10861, 2474, 24501, 4747, 14194, 3258, 29533, 3540, 2850, 9686, 23624, 14213, 15464, 15781, 1999, 11263, 8943, 2850, 1999, 6767, 9336, 3527, 3449, 2396, 1012, 4700, 2620, 4297, 19565, 1017, 3972, 1039, 1012, 1052, 1012, 1052, 1012, 4372, 7913, 27178, 8180, 23391, 3508, 2229, 3449, 28667, 29264, 2063, 23624, 14213, 2696, 1024, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 14405, 2229, 10861, 4487, 13102, 5643, 2099, 8840, 10861, 17254, 2063, 2019, 7934, 2474, 1055, 1012, 1040, 1012, 1050, 2656, 18499, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1010, 4372, 7352, 3540, 10514, 9353, 10383, 10446, 24528, 2474, 10861, 21835, 18678, 1010, 10250, 18513, 28574, 9765, 2050, 6204, 2050, 4372, 3449, 2741, 13820, 2139, 10861, 1025, 1000, 1012, 1012, 3449, 23957, 5802, 2618, 2004, 17897, 14477, 2552, 4183, 6784, 8915, 5017, 10980, 1041, 17727, 28121, 15781, 2632, 9353, 10383, 2099, 2632, 12152, 2139, 2741, 27742, 2139, 3653, 10755, 5555, 3406, 9530, 3050, 4636, 24996, 2891, 10861, 9811, 2063, 2074, 18513, 2906, 14477, 6232, 2050, 1037, 2474, 2741, 27742, 23957, 27266, 2050, 1012, 1012, 1012, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 3449, 13012, 8569, 2532, 16137, 7367, 20704, 24163, 1037, 1025, 1037, 1007, 8291, 1061, 2074, 18513, 2906, 2474, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1012, 1012, 1012, 1012, 1012, 1012, 1038, 1007, 13366, 9013, 3207, 2632, 12152, 2139, 2741, 27742, 1010, 1012, 1012, 1012, 1012, 1012, 1012, 1039, 1007, 2038, 2696, 7367, 2222, 20265, 2632, 8902, 5302, 1012, 1012, 1012, 1012, 1012, 1012, 1010, 1040, 1007, 2053, 27314, 2080, 9152, 3070, 9521, 10975, 5657, 3676, 2139, 2474, 10861, 21835, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 2053, 7367, 2613, 10993, 2080, 2474, 27314, 21736, 2139, 9353, 13094, 3527, 9530, 2474, 2624, 2050, 6232, 2050, 1025, 2474, 27314, 21736, 2139, 5869, 10975, 5657, 22083, 7367, 2613, 10993, 2080, 2139, 9353, 13094, 3527, 9530, 2474, 9530, 8159, 9013, 7405, 3972, 12152, 2139, 2741, 27742, 18499, 4226, 18609, 7367, 11487, 2080, 1010, 13075, 10735, 19960, 10735, 4013, 14479, 27416, 2015, 2053, 7367, 29454, 29206, 7405, 4948, 2566, 2080, 7367, 6449, 3771, 2239, 1061, 7367, 27314, 10464, 2078, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 2139, 9765, 2050, 2433, 2050, 2702, 6633, 2891, 10861, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 9686, 23624, 14213, 15464, 15781, 1999, 11263, 8943, 3527, 1061, 10861, 2053, 7367, 2613, 10993, 2080, 9530, 3366, 10841, 15781, 3672, 2063, 4895, 20302, 17417, 2015, 17540, 2890, 3050, 18975, 2891, 12731, 13728, 3981, 17340, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 10861, 3449, 6020, 9389, 3022, 7367, 3653, 10085, 6279, 2080, 4372, 9353, 8017, 2906, 14057, 13665, 2063, 28681, 2080, 9765, 2080, 1010, 19432, 4372, 19817, 3022, 26775, 12322, 4313, 18609, 9686, 3465, 25438, 2890, 4372, 28681, 3022, 5869, 24501, 4747, 14194, 3258, 2229, 8268, 2229, 1012, 1012, 1012, 24501, 12717, 10497, 2080, 17491, 6633, 2891, 19676, 2906, 12731, 21634, 9033, 9077, 1025, 1015, 1011, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 4487, 25572, 2850, 18499, 3449, 6020, 10482, 20692, 2015, 10514, 12693, 13247, 2229, 3972, 4013, 9623, 2080, 1012, 1012, 1012, 1025, 1016, 1011, 4839, 2063, 10514, 20528, 14693, 2239, 2139, 23408, 5178, 22750, 1010, 10861, 9765, 2050, 7279, 10265, 10111, 1061, 10861, 2053, 9146, 2063, 1010, 7367, 4487, 6593, 2063, 24501, 4747, 14194, 3258, 2632, 12734, 2050, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1017, 1011, 2474, 10514, 20528, 14693, 2239, 2139, 3729, 1051, 4487, 17729, 2618, 9686, 8050, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1018, 1011, 4839, 2368, 10975, 5657, 22083, 2053, 4078, 2923, 8524, 2015, 1010, 2053, 29454, 29206, 7405, 2015, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1019, 1011, 3449, 6020, 7857, 2080, 17540, 2890, 5869, 29454, 29206, 7405, 2015, 14916, 4305, 8883, 1010, 8840, 10861, 2053, 9765, 2050, 9146, 8524, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1020, 1011, 3449, 6020, 2632, 6449, 4313, 2474, 10514, 20528, 14693, 2239, 1061, 4487, 25572, 2099, 2474, 1055, 1012, 1040, 1012, 1050, 5179, 1010, 15699, 3695, 3653, 10755, 5555, 3406, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1021, 1007, 28681, 2080, 7857, 2080, 11865, 2063, 24528, 2474, 10861, 21835, 1010, 23233, 2050, 1037, 5684, 1010, 9033, 6633, 28139, 4372, 24528, 1010, 8254, 6449, 4313, 2474, 10514, 20528, 14693, 2239, 1012, 1012, 1012, 1012, 1012, 1012, 1025, 1022, 1011, 4839, 2063, 4078, 9035, 10446, 3972, 4013, 11788, 27605, 4765, 2080, 1012, 1012, 1012, 1012, 1012, 1025, 1023, 1011, 28681, 2891, 3050, 14405, 26005, 16454, 2229, 3972, 2556, 2063, 4013, 9623, 2080, 1010, 9146, 2063, 28517, 28667, 9236, 2080, 4469, 8551, 3981, 9488, 2139, 14124, 10446, 1012, 1012, 1012, 1012, 1012, 1012, 1000, 1012, 4372, 3539, 2099, 2744, 5740, 9298, 2063, 4078, 2696, 10010, 10861, 3449, 24761, 18532, 2050, 28667, 9236, 20984, 21418, 3217, 3972, 5025, 24761, 18532, 2050, 4013, 9623, 2389, 7367, 19838, 2063, 4054, 3672, 2063, 18499, 3050, 26927, 12273, 11514, 10735, 2139, 23726, 29068, 27893, 1061, 2139, 17062, 2050, 8915, 2278, 12782, 1010, 9530, 8840, 12731, 2389, 3050, 2991, 2891, 2139, 10609, 14262, 17727, 15916, 25389, 2891, 4654, 20464, 2271, 11444, 3672, 2063, 18499, 3050, 19960, 10735, 1061, 5869, 2433, 3022, 17727, 15808, 10230, 18499, 3449, 19429, 14031, 8887, 1012, 4372, 28776, 8915, 28032, 4648, 1010, 17254, 2063, 1041, 25969, 6692, 2099, 4372, 3539, 2099, 11320, 6843, 3449, 9765, 21041, 2080, 2139, 2474, 4748, 15630, 12322, 18622, 14697, 3972, 28667, 9236, 2080, 4748, 14194, 13820, 1012, 3449, 20302, 17417, 2015, 2139, 2474, 4013, 22119, 22750, 7367, 1041, 25969, 6692, 2527, 15219, 3672, 2063, 3948, 1061, 9033, 3449, 28667, 9236, 2080, 5292, 15765, 2080, 6970, 14289, 4355, 2080, 1037, 1007, 4372, 2474, 2433, 2050, 1061, 2744, 5740, 3653, 11020, 28414, 2015, 18499, 2474, 20692, 1038, 1007, 9033, 2474, 24501, 4747, 14194, 3258, 17727, 15916, 25389, 2050, 4830, 11320, 6843, 1037, 3449, 1006, 23726, 29068, 27893, 16429, 15759, 11444, 1007, 1010, 1061, 1025, 1039, 1007, 9033, 11865, 2063, 2139, 8566, 6895, 3527, 18499, 21864, 2368, 5495, 2638, 6178, 6305, 27893, 11498, 3449, 4135, 1006, 23726, 29068, 27893, 4942, 15759, 11444, 1007, 1012, 4372, 12731, 21634, 2632, 3539, 2099, 18975, 2080, 2139, 20302, 17417, 2015, 2139, 2474, 4748, 15630, 12322, 18622, 14697, 1010, 6523, 13820, 1037, 2474, 2433, 2050, 2139, 1012, 1012, 1012, 1012, 1012, 1012, 4654, 5669, 25099, 1024, 1000, 28667, 9236, 2080, 2139, 14124, 10446, 6970, 14289, 4355, 2080, 18499, 3449, 5034, 1012, 1046, 1012, 1039, 1012, 1039, 1012, 1061, 1012, 18499, 4315, 15937, 2080, 17678, 3695, 1061, 8670, 5558, 6986, 3217, 27085, 2080, 3972, 11113, 8649, 1012, 1050, 1012, 1046, 1012, 1054, 1012, 1010, 4372, 2474, 6187, 10383, 1025, 1000, 1037, 1012, 1041, 1012, 1055, 2572, 8189, 4143, 1000, 1050, 5890, 1012, 5890, 1012, 6021, 1012, 5890, 1012, 2268, 1012, 23475, 1000, 1012, 2019, 2080, 2249, 1010, 1050, 5354, 2620, 1010, 1042, 29401, 6421, 1058, 23223, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 6970, 6873, 19570, 3258, 1024, 3449, 2396, 2594, 18845, 4805, 2620, 4372, 16557, 2319, 7405, 9530, 3449, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/peft/peft_model.py\", line 937, in forward\n    return self.base_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 161, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1195, in forward\n    outputs = self.roberta(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 832, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 521, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 410, in forward\n    self_attention_outputs = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 337, in forward\n    self_outputs = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 226, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 23.68 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 20.84 GiB memory in use. Of the allocated memory 18.86 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_checkpoint)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m test_pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtune_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # replace original test labels with predicted labels\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# df_test['label'] = test_pred_labels\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# # save the dataframe with predicted labels to a csv file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(\"Saving predictions to csv...\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# df_test.to_csv('corpus/prediction_task3.tsv', sep='\\t', index=False)\u001b[39;00m\n",
            "File \u001b[0;32m~/tesis/Clasificacion_Sentencias/models/tune_transformer.py:420\u001b[0m, in \u001b[0;36mrun_lora\u001b[0;34m(model_checkpoint, num_labels, train_texts, val_texts, test_texts, y_train, y_val, y_test, hyperparameters)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerifying train dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# This should print the first element to check structure\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict(trainer, test_dataset)\n\u001b[1;32m    423\u001b[0m test_pred_labels \u001b[38;5;241m=\u001b[39m get_labels(predictions)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/peft/peft_model.py\", line 937, in forward\n    return self.base_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 161, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1195, in forward\n    outputs = self.roberta(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 832, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 521, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 410, in forward\n    self_attention_outputs = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 337, in forward\n    self_outputs = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/leon/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 226, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 23.68 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 20.84 GiB memory in use. Of the allocated memory 18.86 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "from models import tune_transformer\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "test_pred_labels = tune_transformer.run_lora(model_checkpoint, 2,\n",
        "                                        train_texts, val_texts, test_texts,\n",
        "                                        y_train, y_val, y_test,\n",
        "                                        hyperparameters=hyperparameters)\n",
        "\n",
        "# # replace original test labels with predicted labels\n",
        "# df_test['label'] = test_pred_labels\n",
        "\n",
        "# # save the dataframe with predicted labels to a csv file\n",
        "# print(\"Saving predictions to csv...\")\n",
        "# df_test.to_csv('corpus/prediction_task3.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Epoch 1/10\n",
            "Batch 0/219, Loss: 0.3163\n",
            "Batch 10/219, Loss: 0.2829\n",
            "Batch 20/219, Loss: 0.6073\n",
            "Batch 30/219, Loss: 0.3850\n",
            "Batch 40/219, Loss: 0.6103\n",
            "Batch 50/219, Loss: 0.4941\n",
            "Batch 60/219, Loss: 0.3568\n",
            "Batch 70/219, Loss: 0.4841\n",
            "Batch 80/219, Loss: 0.3784\n",
            "Batch 90/219, Loss: 0.5119\n",
            "Batch 100/219, Loss: 0.3682\n",
            "Batch 110/219, Loss: 0.3637\n",
            "Batch 120/219, Loss: 0.6363\n",
            "Batch 130/219, Loss: 0.4950\n",
            "Batch 140/219, Loss: 0.2869\n",
            "Batch 150/219, Loss: 0.5203\n",
            "Batch 160/219, Loss: 0.7642\n",
            "Batch 170/219, Loss: 0.4589\n",
            "Batch 180/219, Loss: 0.3738\n",
            "Batch 190/219, Loss: 0.5481\n",
            "Batch 200/219, Loss: 0.4079\n",
            "Batch 210/219, Loss: 0.5875\n",
            "Train Loss: 0.4907\n",
            "Val Accuracy: 80.00% | F1 Score: 0.4444 | Loss: 0.4930\n",
            "\n",
            "Epoch 2/10\n",
            "Batch 0/219, Loss: 0.6267\n",
            "Batch 10/219, Loss: 0.4830\n",
            "Batch 20/219, Loss: 0.7695\n",
            "Batch 30/219, Loss: 0.3313\n",
            "Batch 40/219, Loss: 0.5870\n",
            "Batch 50/219, Loss: 0.4907\n",
            "Batch 60/219, Loss: 0.4633\n",
            "Batch 70/219, Loss: 0.5478\n",
            "Batch 80/219, Loss: 0.6555\n",
            "Batch 90/219, Loss: 0.4949\n",
            "Batch 100/219, Loss: 0.5812\n",
            "Batch 110/219, Loss: 0.4012\n",
            "Batch 120/219, Loss: 0.4232\n",
            "Batch 130/219, Loss: 0.4015\n",
            "Batch 140/219, Loss: 0.7191\n",
            "Batch 150/219, Loss: 0.6099\n",
            "Batch 160/219, Loss: 0.4080\n",
            "Batch 170/219, Loss: 0.2861\n",
            "Batch 180/219, Loss: 0.3692\n",
            "Batch 190/219, Loss: 0.6313\n",
            "Batch 200/219, Loss: 0.5861\n",
            "Batch 210/219, Loss: 0.5819\n",
            "Train Loss: 0.4743\n",
            "Val Accuracy: 79.87% | F1 Score: 0.4568 | Loss: 0.5024\n",
            "\n",
            "Epoch 3/10\n",
            "Batch 0/219, Loss: 0.3072\n",
            "Batch 10/219, Loss: 0.4466\n",
            "Batch 20/219, Loss: 0.4437\n",
            "Batch 30/219, Loss: 0.5665\n",
            "Batch 40/219, Loss: 0.4511\n",
            "Batch 50/219, Loss: 0.6662\n",
            "Batch 60/219, Loss: 0.7610\n",
            "Batch 70/219, Loss: 0.4293\n",
            "Batch 80/219, Loss: 0.2875\n",
            "Batch 90/219, Loss: 0.5923\n",
            "Batch 100/219, Loss: 0.5031\n",
            "Batch 110/219, Loss: 0.6207\n",
            "Batch 120/219, Loss: 0.4793\n",
            "Batch 130/219, Loss: 0.5096\n",
            "Batch 140/219, Loss: 0.3308\n",
            "Batch 150/219, Loss: 0.5942\n",
            "Batch 160/219, Loss: 0.7945\n",
            "Batch 170/219, Loss: 0.3785\n",
            "Batch 180/219, Loss: 0.3960\n",
            "Batch 190/219, Loss: 0.3567\n",
            "Batch 200/219, Loss: 0.4582\n",
            "Batch 210/219, Loss: 0.4172\n",
            "Train Loss: 0.4717\n",
            "Val Accuracy: 80.40% | F1 Score: 0.4588 | Loss: 0.4923\n",
            "\n",
            "Epoch 4/10\n",
            "Batch 0/219, Loss: 0.4503\n",
            "Batch 10/219, Loss: 0.3494\n",
            "Batch 20/219, Loss: 0.5154\n",
            "Batch 30/219, Loss: 0.6554\n",
            "Batch 40/219, Loss: 0.4078\n",
            "Batch 50/219, Loss: 0.4540\n",
            "Batch 60/219, Loss: 0.5615\n",
            "Batch 70/219, Loss: 0.7109\n",
            "Batch 80/219, Loss: 0.4811\n",
            "Batch 90/219, Loss: 0.7991\n",
            "Batch 100/219, Loss: 0.6332\n",
            "Batch 110/219, Loss: 0.1982\n",
            "Batch 120/219, Loss: 0.5672\n",
            "Batch 130/219, Loss: 0.5500\n",
            "Batch 140/219, Loss: 0.2678\n",
            "Batch 150/219, Loss: 0.4570\n",
            "Batch 160/219, Loss: 0.6137\n",
            "Batch 170/219, Loss: 0.4658\n",
            "Batch 180/219, Loss: 0.3958\n",
            "Batch 190/219, Loss: 0.3848\n",
            "Batch 200/219, Loss: 0.4232\n",
            "Batch 210/219, Loss: 0.1776\n",
            "Train Loss: 0.4688\n",
            "Val Accuracy: 80.40% | F1 Score: 0.4588 | Loss: 0.4990\n",
            "\n",
            "Epoch 5/10\n",
            "Batch 0/219, Loss: 0.3630\n",
            "Batch 10/219, Loss: 0.6976\n",
            "Batch 20/219, Loss: 0.6935\n",
            "Batch 30/219, Loss: 0.5013\n",
            "Batch 40/219, Loss: 0.2805\n",
            "Batch 50/219, Loss: 0.2763\n",
            "Batch 60/219, Loss: 0.5759\n",
            "Batch 70/219, Loss: 0.4576\n",
            "Batch 80/219, Loss: 0.3028\n",
            "Batch 90/219, Loss: 0.3798\n",
            "Batch 100/219, Loss: 0.4256\n",
            "Batch 110/219, Loss: 0.6516\n",
            "Batch 120/219, Loss: 0.4661\n",
            "Batch 130/219, Loss: 0.5841\n",
            "Batch 140/219, Loss: 0.7738\n",
            "Batch 150/219, Loss: 0.2267\n",
            "Batch 160/219, Loss: 0.2969\n",
            "Batch 170/219, Loss: 0.5977\n",
            "Batch 180/219, Loss: 0.7772\n",
            "Batch 190/219, Loss: 0.5308\n",
            "Batch 200/219, Loss: 0.4077\n",
            "Batch 210/219, Loss: 0.2789\n",
            "Train Loss: 0.4644\n",
            "Val Accuracy: 80.13% | F1 Score: 0.4578 | Loss: 0.4985\n",
            "\n",
            "Epoch 6/10\n",
            "Batch 0/219, Loss: 0.4717\n",
            "Batch 10/219, Loss: 0.3575\n",
            "Batch 20/219, Loss: 0.3971\n",
            "Batch 30/219, Loss: 0.6387\n",
            "Batch 40/219, Loss: 0.4632\n",
            "Batch 50/219, Loss: 0.4541\n",
            "Batch 60/219, Loss: 0.5559\n",
            "Batch 70/219, Loss: 0.2829\n",
            "Batch 80/219, Loss: 0.4954\n",
            "Batch 90/219, Loss: 0.3948\n",
            "Batch 100/219, Loss: 0.6816\n",
            "Batch 110/219, Loss: 0.3880\n",
            "Batch 120/219, Loss: 0.2739\n",
            "Batch 130/219, Loss: 0.3775\n",
            "Batch 140/219, Loss: 0.5943\n",
            "Batch 150/219, Loss: 0.3078\n",
            "Batch 160/219, Loss: 0.6280\n",
            "Batch 170/219, Loss: 0.5124\n",
            "Batch 180/219, Loss: 0.6885\n",
            "Batch 190/219, Loss: 0.5625\n",
            "Batch 200/219, Loss: 0.5467\n",
            "Batch 210/219, Loss: 0.4709\n",
            "Train Loss: 0.4628\n",
            "Val Accuracy: 80.27% | F1 Score: 0.4583 | Loss: 0.5070\n",
            "\n",
            "Epoch 7/10\n",
            "Batch 0/219, Loss: 0.4380\n",
            "Batch 10/219, Loss: 0.5434\n",
            "Batch 20/219, Loss: 0.3614\n",
            "Batch 30/219, Loss: 0.5625\n",
            "Batch 40/219, Loss: 0.5517\n",
            "Batch 50/219, Loss: 0.5293\n",
            "Batch 60/219, Loss: 0.3843\n",
            "Batch 70/219, Loss: 0.4645\n",
            "Batch 80/219, Loss: 0.4775\n",
            "Batch 90/219, Loss: 0.7045\n",
            "Batch 100/219, Loss: 0.5207\n",
            "Batch 110/219, Loss: 0.4428\n",
            "Batch 120/219, Loss: 0.2688\n",
            "Batch 130/219, Loss: 0.5414\n",
            "Batch 140/219, Loss: 0.3841\n",
            "Batch 150/219, Loss: 0.5156\n",
            "Batch 160/219, Loss: 0.4471\n",
            "Batch 170/219, Loss: 0.6294\n",
            "Batch 180/219, Loss: 0.5717\n",
            "Batch 190/219, Loss: 0.4481\n",
            "Batch 200/219, Loss: 0.3883\n",
            "Batch 210/219, Loss: 0.7586\n",
            "Train Loss: 0.4583\n",
            "Val Accuracy: 79.20% | F1 Score: 0.4543 | Loss: 0.5041\n",
            "\n",
            "Epoch 8/10\n",
            "Batch 0/219, Loss: 0.4486\n",
            "Batch 10/219, Loss: 0.5904\n",
            "Batch 20/219, Loss: 0.6359\n",
            "Batch 30/219, Loss: 0.4800\n",
            "Batch 40/219, Loss: 0.3116\n",
            "Batch 50/219, Loss: 0.1525\n",
            "Batch 60/219, Loss: 0.2155\n",
            "Batch 70/219, Loss: 0.3961\n",
            "Batch 80/219, Loss: 0.5385\n",
            "Batch 90/219, Loss: 0.6665\n",
            "Batch 100/219, Loss: 0.2951\n",
            "Batch 110/219, Loss: 0.3066\n",
            "Batch 120/219, Loss: 0.5720\n",
            "Batch 130/219, Loss: 0.5770\n",
            "Batch 140/219, Loss: 0.5775\n",
            "Batch 150/219, Loss: 0.3964\n",
            "Batch 160/219, Loss: 0.5856\n",
            "Batch 170/219, Loss: 0.7057\n"
          ]
        }
      ],
      "source": [
        "from transformers import MambaForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# import f1_score from sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "# import Loading Bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "max_length = 2048\n",
        "\n",
        "# Create a SummaryWriter to log metrics\n",
        "writer = SummaryWriter()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Define Dataset Class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        inputs = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove batch dimension\n",
        "        inputs[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "# 2. Modify the model to add a classification head\n",
        "class MambaForTextClassification(nn.Module):\n",
        "    def __init__(self, model, num_labels):\n",
        "        super(MambaForTextClassification, self).__init__()\n",
        "        self.mamba_model = model\n",
        "        self.classifier = nn.Linear(self.mamba_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get hidden states from the language model\n",
        "        outputs = self.mamba_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
        "\n",
        "        # Pool the hidden states (take the hidden state corresponding to [CLS] token or mean pooling)\n",
        "        pooled_output = hidden_states[:, 0, :]  # Using the first token's embedding (usually [CLS] token)\n",
        "\n",
        "        # Pass the pooled output through the classifier\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# 3. Initialize model, tokenizer, and dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
        "mamba_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
        "classification_model = MambaForTextClassification(mamba_model, num_labels=2)\n",
        "classification_model = nn.DataParallel(classification_model)\n",
        "classification_model.to(device)\n",
        "\n",
        "freeze_mamba = False\n",
        "if freeze_mamba:\n",
        "    for param in classification_model.module.mamba_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Tokenize and create dataset\n",
        "train_dataset = TextDataset(train_texts.tolist(), y_train.tolist(), tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TextDataset(val_texts.tolist(), y_val.tolist(), tokenizer, max_length)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TextDataset(test_texts.tolist(), y_test.tolist(), tokenizer, max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 4. Define optimizer and loss function\n",
        "optimizer = optim.AdamW(classification_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. Evaluation function\n",
        "def evaluate_model(model, dataloader, epoch, phase='val'):\n",
        "    if phase == 'test':\n",
        "        print(\"\\nEvaluating on test data...\")\n",
        "\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch.get('attention_mask').to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probs, dim=-1)\n",
        "            \n",
        "            correct_predictions += (predicted_class == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted_class.cpu().numpy())\n",
        "            \n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    # Log validation loss, accuracy, and f1 score\n",
        "    writer.add_scalar(f'{phase}/Loss', average_loss, epoch)\n",
        "    writer.add_scalar(f'{phase}/Accuracy', accuracy, epoch)\n",
        "    writer.add_scalar(f'{phase}/F1_Score', f1, epoch)\n",
        "\n",
        "    print(f\"{phase.capitalize()} Accuracy: {accuracy * 100:.2f}% | F1 Score: {f1:.4f} | Loss: {average_loss:.4f}\")\n",
        "    if phase == 'test':\n",
        "        # print predictions\n",
        "        print(\"Predictions:\", all_predictions)\n",
        "\n",
        "# 5. Training loop\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        model.train()\n",
        "        loss_sum = 0\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch.get('attention_mask').to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            # Log loss\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print batch progress\n",
        "            if batch_idx % 10 == 0:  # Print every 10 batches\n",
        "                print(f\"Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
        "\n",
        "        print(f\"Train Loss: {loss_sum/len(train_dataloader):.4f}\")\n",
        "        \n",
        "        # calculate validation accuracy after each epoch\n",
        "        evaluate_model(model, val_dataloader, epoch, phase='val')\n",
        "\n",
        "# Train the model\n",
        "train_model(classification_model, train_dataloader, val_dataloader, optimizer, criterion, epochs)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "evaluate_model(classification_model, test_dataloader, epoch=-1, phase='test')\n",
        "\n",
        "# Close TensorBoard writer\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print End Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "End-Time\n",
            "2024-09-17 01:07:12\n",
            "------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print_time.print_(\"End-Time\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
