{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Erfedieyte, Confidence: 0.07662170001808742\n",
      "Text: 'Pacuhsu, Confidence: 0.04445684126779143\n",
      "Text: Casacion WTerJESTO FoR L AOG, Confidence: 0.023390856621495648\n",
      "Text: Flvna, Confidence: 0.0184925436535903\n",
      "Text: MIERES DeNs En, Confidence: 0.1797135026074586\n",
      "Text: Clusa, Confidence: 0.48221780338085524\n",
      "Text: Au3io, Confidence: 0.004049417263325394\n",
      "Text: Nng?, Confidence: 0.08864929527044296\n",
      "Text: corte SUpReia DE Justicia, Confidence: 0.1287975267054687\n",
      "Text: Vl, Confidence: 0.025908018838894802\n",
      "Text: CLEMFNTF, Confidence: 0.15277993850039756\n",
      "Text: MUNEZ   VELNZOIEZ, Confidence: 0.24620373011194396\n",
      "Text: Hoyic Cio, Confidence: 0.3603642474958184\n",
      "Text: rokrahabinae, Confidence: 0.0021414905201152783\n",
      "Text: Dj1nso' K3aW00, Confidence: 0.006277298496225054\n",
      "Text: 41 , Confidence: 0.0725625067339076\n",
      "Text: anoGrwio, Confidence: 0.0039769158570148325\n",
      "Text: ACUERDO, Confidence: 0.9988886980058934\n",
      "Text: SENTENCIA, Confidence: 0.9890358168065657\n",
      "Text: MÚMEROE/uMeneh, Confidence: 0.029705696794201618\n",
      "Text: Xaye ?ot, Confidence: 0.09556028678074789\n",
      "Text: Ciudar, Confidence: 0.9223919638551702\n",
      "Text: Asuncicn; Capital, Confidence: 0.9372025868825419\n",
      "Text: Reputhica del, Confidence: 0.2984680559502205\n",
      "Text: @l<_, Confidence: 0.05313661350311366\n",
      "Text: 264, Confidence: 0.13045334801582573\n",
      "Text: {tes, Confidence: 0.1365710347890854\n",
      "Text: Jño   dos mt, Confidence: 0.38335246344934787\n",
      "Text: Qrlre, Confidence: 0.10864748566561029\n",
      "Text: Estz70o, Confidence: 0.06055020354711834\n",
      "Text: reur dcs, Confidence: 0.17904768762078846\n",
      "Text: Saa e Acuerdos ks seicres Hinistros oe, Confidence: 0.3941834248683553\n",
      "Text: Excelentisima Corte Sucrema de, Confidence: 0.7425088809168842\n",
      "Text: Jusuca, Confidence: 0.5028456402024278\n",
      "Text: ALICIA BEATRIZ PUCHETA DE CORREA,, Confidence: 0.7165260971082443\n",
      "Text: SINDULFO BLANCO, Confidence: 0.5504759664563126\n",
      "Text: LUIS   MARIA, Confidence: 0.7405402445164643\n",
      "Text: ENITEZ RIERA , arte mi la Secretara aulcnzante, Confidence: 0.5837905868524939\n",
      "Text: expediente carallladc: \"RECURSO DE, Confidence: 0.26226571773749896\n",
      "Text: CASACION INTERPUESTO POR, Confidence: 0.5524655373655064\n",
      "Text: ABOG, ELMRA MIERES DENS EN LA Causa; \"MP, Confidence: 0.33309194615936566\n",
      "Text: AURELIO, Confidence: 0.9958105239168216\n",
      "Text: NUNEZ, Confidence: 0.43084978782777694\n",
      "Text: AovET, Confidence: 0.30143506245798385\n",
      "Text: CLEIENTE NUREZ VELAZQUEZ , Confidence: 0.7249048717418753\n",
      "Text: HoMIcIDIO DOLOSO', Confidence: 0.2685505196964277\n",
      "Text: conlta, Confidence: 0.15185576276585475\n",
      "Text: Acucrdo y, Confidence: 0.4128119363147143\n",
      "Text: Sentencia N, Confidence: 0.5749358592846685\n",
      "Text: de techa J0 de atril de 2010 dictado pc, Confidence: 0.5362509690006927\n",
      "Text: Iohinal, Confidence: 0.2884472029363872\n",
      "Text: Apelacicn en, Confidence: 0.5034108415205185\n",
      "Text: Cvil,, Confidence: 0.825079334581199\n",
      "Text: Comercial, Latcral, Confidence: 0.838875925798533\n",
      "Text: Penal, Segunda Sala Saa ce, Confidence: 0.78815386938608\n",
      "Text: Circurscripcicn Judicial de Caaguazu;, Confidence: 0.5545219388388197\n",
      "Text: Previo estudio de, Confidence: 0.7565209209350677\n",
      "Text: antecejentes, Confidence: 0.701644197553759\n",
      "Text: Excelertisima Corte Suprema de Jusbcia;, Confidence: 0.6034797825170226\n",
      "Text: resolvis plartear |, Confidence: 0.5857545802772539\n",
      "Text: siguientes;, Confidence: 0.693976517029961\n",
      "Text: CuESTiones:, Confidence: 0.2075173144983434\n",
      "Text: LEs admisible, Confidence: 0.8665575580851589\n",
      "Text: Recurso de Casación Interpuesto? ,, Confidence: 0.7184331947553702\n",
      "Text: caso, Confidence: 0.9809155464172363\n",
      "Text: Zresulta proccdente? ., Confidence: 0.6010571638308572\n",
      "Text: Jos electcs, Confidence: 0.5930202950725689\n",
      "Text: oeleuina, Confidence: 0.38984726881660864\n",
      "Text: Jgden, Confidence: 0.021445170030786425\n",
      "Text: paa, Confidence: 0.9533957068723734\n",
      "Text: Cxlcsicion De la5 ociniones ,, Confidence: 0.23843125806491758\n",
      "Text: fealzb, Confidence: 0.11745033480105495\n",
      "Text: EoTedoi, Confidence: 0.015096533852593453\n",
      "Text: ancjc, Confidence: 0.7972659369940935\n",
      "Text: siguiente resultacc, Confidence: 0.8913824052239733\n",
      "Text: BLANCO, PUCHETA DE CORREA, Confidence: 0.8109797117058688\n",
      "Text: BENITEZ~, Confidence: 0.5757696281053274\n",
      "Text: LA PRIIERA CUESTION PLANTEADA, Confidence: 0.5280422820168968\n",
      "Text: MINISTRO BLANCO dijo: Que por Acuerdo y, Confidence: 0.3628524422986627\n",
      "Text: Sentencia, Confidence: 0.9653569133112967\n",
      "Text: 13 dc {echa J0 de abrl de 2010  dictada por, Confidence: 0.3573441933353167\n",
      "Text: Triburd, Confidence: 0.47997046366542984\n",
      "Text: Apelaciones, Confidence: 0.9245772008738485\n",
      "Text: Cvil,, Confidence: 0.5339673726081614\n",
      "Text: Comercial, Confidence: 0.9989751732804246\n",
      "Text: Laboral Penal; Sejunda Sda de, Confidence: 0.5347401693146223\n",
      "Text: Clrcunscrircibn Juocial, Confidence: 0.3681898937585433\n",
      "Text: Caaguazú;, Confidence: 0.6649929874676357\n",
      "Text: Doracual, Confidence: 0.11792941859691186\n",
      "Text: fesonio entte Dtas (osas, Confidence: 0.16744844541528225\n",
      "Text: CoNFIRMAR, Confidence: 0.777934726997286\n",
      "Text: Senencia Delnibva L, Confidence: 0.45308979722991755\n",
      "Text: Ce fecha 23, Confidence: 0.8648417023118381\n",
      "Text: mayo, Confidence: 0.7750248312950134\n",
      "Text: 2008, ANOTAR, Confidence: 0.9223516646040757\n",
      "Text: Abogaja Elvira Mieres Deris, Inlerpone recursp extracrcinarvo, Confidence: 0.2167247549533085\n",
      "Text: casacicn ccnta el lalb, Confidence: 0.4136339108573771\n",
      "Text: dictaco |o, Confidence: 0.13066994479158361\n",
      "Text: 'Tnibunal, Confidence: 0.6299826033150988\n",
      "Text: Alzada, Confidence: 0.9965148074457513\n",
      "Text: fecufienc, Confidence: 0.0852527925436225\n",
      "Text: cenua, Confidence: 0.5709177597669219\n",
      "Text: agravios   manifoslanc, Confidence: 0.5131728386310541\n",
      "Text: Resolucen   alacada, Confidence: 0.22163034083856026\n",
      "Text: manifiestamente intundada_, Confidence: 0.9147429437211146\n",
      "Text: 'prirat, Confidence: 0.47841133887343834\n",
      "Text: termino cabe destacar que, Confidence: 0.5270570075964534\n",
      "Text: svo denlro del actual sistena procesal, Confidence: 0.5929827782036201\n",
      "Text: Irents por ks principics de taratividad, Confidence: 0.3696734161946216\n",
      "Text: ndateonica; con [, Confidence: 0.2532738482197502\n",
      "Text: cual los falkos, Confidence: 0.43721186882746393\n",
      "Text: auo, Confidence: 0.079850078720231\n",
      "Text: esaci, Confidence: 0.02703691490745395\n",
      "Text: 744., Confidence: 0.25240078568458557\n",
      "Text: 8, Confidence: 0.11109587933505338\n",
      "Text: Paaquay, Confidence: 0.5402249695957673\n",
      "Text: 2035, Confidence: 0.24362578988075256\n",
      "Text: 98912, Confidence: 0.04014324676352104\n",
      "Text: SFD, Confidence: 0.16000783364726145\n",
      "Text: Juueay ', Confidence: 0.19116429219366687\n",
      "Text: Sisieiny, Confidence: 0.20722445499805964\n",
      "Text: ULanco), Confidence: 0.25549498617251304\n",
      "Text: SiEuLFO, Confidence: 0.10441440477620104\n",
      "Text: \"MansirD, Confidence: 0.014995308185860977\n",
      "Text: CuBkEA, Confidence: 0.2126705021358127\n",
      "Text: Alkzapichcta&, Confidence: 0.04947627436706569\n",
      "Text: 1ansua, Confidence: 0.03732125651845257\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "reader = easyocr.Reader(['es'])\n",
    "\n",
    "# Perform OCR on an image\n",
    "image_path = 'caso_legal.png'\n",
    "image = cv2.imread(image_path)\n",
    "results = reader.readtext(image)\n",
    "\n",
    "# Print the extracted text\n",
    "for (bbox, text, confidence) in results:\n",
    "    print(f\"Text: {text}, Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to infer channel dimension format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m processor \u001b[38;5;241m=\u001b[39m TrOCRProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/trocr-base-printed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/trocr-base-printed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpixel_values\n\u001b[1;32m     12\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(pixel_values)\n\u001b[1;32m     13\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:85\u001b[0m, in \u001b[0;36mTrOCRProcessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `images` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:247\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    240\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# We assume that all images have the same channel dimension format.\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    250\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize_dict, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    253\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_utils.py:254\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image, num_channels)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[last_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer channel dimension format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to infer channel dimension format"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "# load image from the IAM database (actually this model is meant to be used on printed text)\n",
    "# url = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\n",
    "image = Image.open('caso_legal.png').convert(\"RGB\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: \n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Open the image\n",
    "image = Image.open('caso_legal.png').convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Generate the text\n",
    "generated_ids = model.generate(pixel_values, max_length=512, num_beams=3)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Extracted Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/leon/.keras-ocr/craft_mlt_25k.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 11:40:12.565616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21542 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-09-20 11:40:12.566360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22479 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:73:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Dense: {'weights': [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]], dtype=float32), array([1., 0., 0., 0., 1., 0.], dtype=float32)]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_ocr\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_ocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Path to your image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaso_legal.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_ocr/pipeline.py:22\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, detector, recognizer, scale, max_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     detector \u001b[38;5;241m=\u001b[39m detection\u001b[38;5;241m.\u001b[39mDetector()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recognizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     recognizer \u001b[38;5;241m=\u001b[39m \u001b[43mrecognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m scale\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector \u001b[38;5;241m=\u001b[39m detector\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_ocr/recognition.py:388\u001b[0m, in \u001b[0;36mRecognizer.__init__\u001b[0;34m(self, alphabet, weights, build_params)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet \u001b[38;5;241m=\u001b[39m alphabet\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblank_label_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(alphabet)\n\u001b[1;32m    383\u001b[0m (\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_model,\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_model,\n\u001b[0;32m--> 388\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphabet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphabet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     weights_dict \u001b[38;5;241m=\u001b[39m PRETRAINED_WEIGHTS[weights]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_ocr/recognition.py:277\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(alphabet, height, width, color, filters, rnn_units, dropout, rnn_steps_to_discard, pool_size, stn)\u001b[0m\n\u001b[1;32m    275\u001b[0m locnet_y \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten()(locnet_y)\n\u001b[1;32m    276\u001b[0m locnet_y \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(locnet_y)\n\u001b[0;32m--> 277\u001b[0m locnet_y \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(locnet_y)\n\u001b[1;32m    284\u001b[0m localization_net \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mstn_input_layer, outputs\u001b[38;5;241m=\u001b[39mlocnet_y)\n\u001b[1;32m    285\u001b[0m x \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(_transform, output_shape\u001b[38;5;241m=\u001b[39mstn_input_output_shape)(\n\u001b[1;32m    286\u001b[0m     [x, localization_net(x)]\n\u001b[1;32m    287\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     75\u001b[0m     units,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     87\u001b[0m ):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_regularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m=\u001b[39m units\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39mget(activation)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Dense: {'weights': [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]], dtype=float32), array([1., 0., 0., 0., 1., 0.], dtype=float32)]}"
     ]
    }
   ],
   "source": [
    "import keras_ocr\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "# Path to your image\n",
    "image_path = 'caso_legal.png'\n",
    "\n",
    "# Read the image\n",
    "image = keras_ocr.tools.read(image_path)\n",
    "\n",
    "# Perform OCR\n",
    "predictions = pipeline.recognize([image])\n",
    "\n",
    "# Print the predictions\n",
    "for prediction in predictions[0]:\n",
    "    print(f\"Text: {prediction[0]}, Bounding Box: {prediction[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.ucaslcl.GOT-OCR2_0.cf6b7386bc89a54f09785612ba74cb12de6fa17c.modeling_GOT.GOTConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mucaslcl/GOT-OCR2_0\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mucaslcl/GOT-OCR2_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# input your test image\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.ucaslcl.GOT-OCR2_0.cf6b7386bc89a54f09785612ba74cb12de6fa17c.modeling_GOT.GOTConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'ucaslcl/GOT-OCR2_0',\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='cuda',\n",
    "    use_safetensors=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model = model.eval().cuda()\n",
    "\n",
    "# input your test image\n",
    "image_file = 'caso_legal.png'\n",
    "\n",
    "# Perform OCR\n",
    "res = model.chat(tokenizer, image_file, ocr_type='ocr')\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORTE SUPREMA DE JUSTICIA\n",
      "\"Elghted feedencia Nacional: 1811-2011\"\n",
      "RECIBIDO\n",
      "11 MAY 2011\n",
      "Roque per\n",
      "S.F.DE.P.J\n",
      "EXPEDIENTE: \"RECURSO DE CASACIÓN INTERPUESTO POR LA ABOG.\n",
      "ELVIRA MIERES DENIS EN LA CAUSA: \"M.P. CJ AURELIO NUÑEZ\n",
      "VELAZQUEZ Y CLEMENTE NUÑEZ VELAZQUEZ S HOMICIDIO\n",
      "DOLOSO\" Año 2010, N° 412, Folio 67 vita-\n",
      "ACUERDO Y SENTENCIA NÚMEROlute nomuta yocho.\"\n",
      "En la Ciudad de Asunción, Capital de la República del Paraguay, a los...\n",
      "nes de...... May......\n",
      "del año dos m...\n",
      "dias,\n",
      "estando\n",
      "reunidos en la Sala de Acuerdos los señores Ministros de la Excelentisima Corte Suprema de\n",
      "Justicia, ALICIA BEATRIZ PUCHETA DE CORREA, SINDULFO BLANCO Y LUIS MARIA\n",
      "BENITEZ RIERA, ante mi la Secretaria autorizante, se trajo el expediente caratulado: \"RECURSO DE\n",
      "CASACIÓN INTERPUESTO POR LA ABOG. ELVIRA MIERES DENIS EN LA CAUSA: \"M.P. C/ AURELIO\n",
      "NUÑEZ VELAZQUEZ Y CLEMENTE NUÑEZ VELAZQUEZ SI HOMICIDIO DOLOSO\", contra el Acuerdo y\n",
      "Sentencia N° 13 de fecha 30 de abril de 2010 dictado por el Tribunal de Apelación en lo Civil,\n",
      "Comercial, Laboral y Penal, Segunda Sala Sala de la Circunscripción Judicial de Caaguazú.\n",
      "Previo estudio de los antecedentes del caso, la Excelentisima Corte Suprema de Justicia,\n",
      "resolvió plantear las siguientes:-\n",
      "CUESTIONES:\n",
      "¿Es admisible el Recurso de Casación interpuesto?.-\n",
      "En su caso, ¿resulta procedente?.-\n",
      "A los efectos de determinar un orden para la exposición de las opiniones, se realizó un\n",
      "sorteo que arrojó el siguiente resultado: BLANCO, PUCHETA DE CORREA Y BENITEZ-\n",
      "A LA PRIMERA CUESTIÓN PLANTEADA, el MINISTRO BLANCO dijo: Que por Acuerdo y\n",
      "Sentencia N° 13 de fecha 30 de abril de 2010, dictada por el Tribunal de Apelaciones en lo Civil,\n",
      "Comercial, Laboral Penal, Segunda Sala, de la Circunscripción Judicial de Caaguazú, por la cual se\n",
      "resolvió entre otras cosas: \"... CONFIRMAR la Sentencia Definitiva N° 40 de fecha 23 de mayo de\n",
      "2008, ANOTAR.....\n",
      "La Abogada Elvira Mieres Denis, interpone recurso extraordinario de casación contra el fallo\n",
      "dictado por el Tribunal de Alzada.-\n",
      "La recurrente centra sus agravios manifestande que la Resolución atacada es\n",
      "manifiestamente infundada) -\n",
      "Secretaria\n",
      "Abg. Karinna To\n",
      "En primer término cabe destacar que el sistema recursivo dentro del actual sistema procesal\n",
      "palmente por los principios de taxatividad y debida,tonica, con lo cual los fallos\n",
      "Luis Merja Paniz\n",
      "Ministro\n",
      "SINDULFO BLANCO\n",
      "Ministro\n",
      "ALICIA PUCHETA de CORREA\n",
      "Ministra\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "import io\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Explicitly load the credentials\n",
    "credentials = service_account.Credentials.from_service_account_file('/home/leon/tesis/Clasificacion_Sentencias/GoogleCloudKey/strong-skyline-423405-a8-52bcbd50a7e1.json')\n",
    "\n",
    "# Set up Google Cloud Vision client\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "# Load the image file\n",
    "image_path = 'caso_legal.png'\n",
    "with io.open(image_path, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.Image(content=content)\n",
    "\n",
    "# Perform OCR\n",
    "response = client.text_detection(image=image)\n",
    "text = response.full_text_annotation\n",
    "\n",
    "print(text.text)\n",
    "\n",
    "# Print the extracted text\n",
    "# for text in texts:\n",
    "#     print(text.description)\n",
    "\n",
    "if response.error.message:\n",
    "    raise Exception(f'{response.error.message}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d2ba8f1dfe4749bd5151f207443e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=torch.float16,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 992\n",
      "\n",
      " \n",
      "El Ministerio Fiscal no presentó oposición.\n",
      "La defensa de la recurrente, mediante un escrito presentado a dicha fecha, manifestó que reitera\n",
      "la posición tomada desde el inicio del juicio, de no reconocer la jurisdicción del Tribunal de Apelaciones\n",
      "en materia laboral, ya que el delito en cuestión no se produjo en el domicilio de los acusados.\n",
      "Ministro Benitez\n",
      "Ministro\n",
      "LUIS MARIA BENITEZ RIERA\n",
      "Concluyendo, el Ministerio Fiscal presentó sus argumentos a favor de que se declara\n",
      "inadmisible el recurso de casación debido a:\n",
      "1. Que la instancia de segunda instancia no ha producido \"juicio\" (juris consulto) al recurrente,\n",
      "porque el Acuerdo y Sentencia se dictaron a título provisional \"-\n",
      "2. La Resolución atacada no contiene \"nuevas aplicaciones de la ley\" que justifiquen el recurso de\n",
      "casación.-\n",
      "En la sentencia dictada por el Tribunal de Instancia, se manifestó que: \"El recurso de casación se\n",
      "deberá recibir y tratar, no obstante que se presentó contra un fallo dictado a título provisional, siempre\n",
      "que se le considere que el recurso plantea materias de ley nuevas o que afectan al derecho constituensional\n",
      "o a los principios fundamentales del sistema judicial, como la fiscalía, la justicia o el juego (art. 5.º,\n",
      "cap. I, ley 3.643/82).\"\n",
      "Cabe mencionar que ambos partes reconocen que, a efectos de establecer lo correspondiente a la\n",
      "fecha, desde el principio del proceso, debe hacerse referencia a la Resolución Definitiva dictada por el\n",
      "Tribunal de Instancia el 23 de Mayo de 2008.\n",
      "\n",
      "The Supreme Court of Justice\n",
      "\"Elghted feedencia Nacional: 1811-2011\"\n",
      "RECEIVED\n",
      "11 MAY 2011\n",
      "Roque Per\n",
      "S.F.DE.P.J\n",
      "FILE NUMBER: \"RECURSO DE CASACIÓN FILED BY ATTORNEY ELVIRA MIERES DENIS IN CASE: \"M.P. CJ AURELIO NUÑEZ VELAZQUEZ AND CLEMENTE NUÑEZ VELAZQUEZ FOR HOMICIDE WITH MALICE\" YEAR 2010, CAUSE NBER 412, FOLIO 67\n",
      "ACORD AND SENTENCING NUMBER \"lute nomuta yocho.\"\n",
      "In the City of Asunción, Capital of the Republic of Paraguay, on the twenty-third day of the month of [blank], in the year two thousand and ten,\n",
      "being convened in the Sala de Acuerdos the honorable Justices of the Supreme Court of Justice, Alicia Beatriz Pucheta Correa, Sindulfo Blanco, and Luis Maria Benitez Riera, in my presence as the authorizing secretary, the following file was brought in with the title: \"RECURSO DE CASACIÓN FILED BY ATTORNEY ELVIRA MIERES DENIS IN CASE: \"M.P. CJ AURELIO NUÑEZ VELAZQUEZ AND CLEMENTE NUÑEZ VELAZQUEZ FOR HOMICIDE WITH MALICE\", against the Acord and Sentencing Number 13, dated 30 April 2010, issued by the Appeals Court in Civil, Commercial, Labor, and Penal Matters, Second Sala, Circuit Judiciary of Caaguazú.\n",
      "\n",
      "After studying the case materials, the Supreme Court of Justice resolved to consider the following issues:\n",
      "\n",
      "QUESTIONS:\n",
      "Is the Recurso de Casación admissible?.-\n",
      "In this respect, is it proper?.-\n",
      "\n",
      "To determine the order of opinion presentations, a drawing was held, yielding the following result: Blanco, Pucheta Correa, Benitez\n",
      "\n",
      "Regarding the first question posed, Minister Blanco stated that by Acord and Sentencing Number 13, dated\n"
     ]
    }
   ],
   "source": [
    "# Define input text\n",
    "input_text = \"Re-escribe el siguiente texto jurídico con el objetivo de limpiar los errores de OCR mientras mantienes el contenido original en español:\\n\\n\" + text.text\n",
    "\n",
    "# Tokenize input text\n",
    "tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(\"Number of tokens:\", len(tokens[0]))\n",
    "\n",
    "# Generate output text\n",
    "generated_ids = model.generate(tokens, max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# Slice generated_ids to remove the input tokens\n",
    "generated_only_ids = generated_ids[0, len(tokens[0]):]\n",
    "\n",
    "# Decode only the newly generated tokens\n",
    "result = tokenizer.decode(generated_only_ids, skip_special_tokens=True)\n",
    "print('\\n', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 00:28:29.786184: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-20 00:28:29.833953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-20 00:28:30.658714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9ab527ad1a409bb7f49e8d7458812c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ee9d357b74498e99a39856955da768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9597e9d1c542fc8c060909f28ed136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506625296cb94b7bbef3a6873c954e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692a6c75d9264f16b4869c3cf4513918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0c29bd4ce3463493d911726038b9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d677866cfb4252a10a9852068cf55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d005b775f0704a7f9c34753ffdb0103c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3da9fd57f5f45d4ab7b26baf8f74cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de1828f6be4e80872e102251208cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cb9a7cb0d24c5b93d63d740e45b680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/leon/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hey how are you doing today? I’m doing great. I’m a little tired, but'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-escribe el siguiente texto jurídico con el objetivo de limpiar los errores de OCR mientras mantienes el contenido original en español:\n",
      "\n",
      "CORTE SUPREMA DE JUSTICIA\n",
      "\"Elghted feedencia Nacional: 1811-2011\"\n",
      "RECIBIDO\n",
      "11 MAY 2011\n",
      "Roque per\n",
      "S.F.DE.P.J\n",
      "EXPEDIENTE: \"RECURSO DE CASACIÓN INTERPUESTO POR LA ABOG.\n",
      "ELVIRA MIERES DENIS EN LA CAUSA: \"M.P. CJ AURELIO NUÑEZ\n",
      "VELAZQUEZ Y CLEMENTE NUÑEZ VELAZQUEZ S HOMICIDIO\n",
      "DOLOSO\" Año 2010, N° 412, Folio 67 vita-\n",
      "ACUERDO Y SENTENCIA NÚMEROlute nomuta yocho.\"\n",
      "En la Ciudad de Asunción, Capital de la República del Paraguay, a los...\n",
      "nes de...... May......\n",
      "del año dos m...\n",
      "dias,\n",
      "estando\n",
      "reunidos en la Sala de Acuerdos los señores Ministros de la Excelentisima Corte Suprema de\n",
      "Justicia, ALICIA BEATRIZ PUCHETA DE CORREA, SINDULFO BLANCO Y LUIS MARIA\n",
      "BENITEZ RIERA, ante mi la Secretaria autorizante, se trajo el expediente caratulado: \"RECURSO DE\n",
      "CASACIÓN INTERPUESTO POR LA ABOG. ELVIRA MIERES DENIS EN LA CAUSA: \"M.P. C/ AURELIO\n",
      "NUÑEZ VELAZQUEZ Y CLEMENTE NUÑEZ VELAZQUEZ SI HOMICIDIO DOLOSO\", contra el Acuerdo y\n",
      "Sentencia N° 13 de fecha 30 de abril de 2010 dictado por el Tribunal de Apelación en lo Civil,\n",
      "Comercial, Laboral y Penal, Segunda Sala Sala de la Circunscripción Judicial de Caaguazú.\n",
      "Previo estudio de los antecedentes del caso, la Excelentisima Corte Suprema de Justicia,\n",
      "resolvió plantear las siguientes:-\n",
      "CUESTIONES:\n",
      "¿Es admisible el Recurso de Casación interpuesto?.-\n",
      "En su caso, ¿resulta procedente?.-\n",
      "A los efectos de determinar un orden para la exposición de las opiniones, se realizó un\n",
      "sorteo que arrojó el siguiente resultado: BLANCO, PUCHETA DE CORREA Y BENITEZ-\n",
      "A LA PRIMERA CUESTIÓN PLANTEADA, el MINISTRO BLANCO dijo: Que por Acuerdo y\n",
      "Sentencia N° 13 de fecha 30 de abril de 2010, dictada por el Tribunal de Apelaciones en lo Civil,\n",
      "Comercial, Laboral Penal, Segunda Sala, de la Circunscripción Judicial de Caaguazú, por la cual se\n",
      "resolvió entre otras cosas: \"... CONFIRMAR la Sentencia Definitiva N° 40 de fecha 23 de mayo de\n",
      "2008, ANOTAR.....\n",
      "La Abogada Elvira Mieres Denis, interpone recurso extraordinario de casación contra el fallo\n",
      "dictado por el Tribunal de Alzada.-\n",
      "La recurrente centra sus agravios manifestande que la Resolución atacada es\n",
      "manifiestamente infundada) -\n",
      "Secretaria\n",
      "Abg. Karinna To\n",
      "En primer término cabe destacar que el sistema recursivo dentro del actual sistema procesal\n",
      "palmente por los principios de taxatividad y debida,tonica, con lo cual los fallos\n",
      "Luis Merja Paniz\n",
      "Ministro\n",
      "SINDULFO BLANCO\n",
      "Ministro\n",
      "ALICIA PUCHETA de CORREA\n",
      "Ministra\n",
      "Sentencia\n",
      "Corte Suprema de Justicia\n",
      "MINISTERIO PÚBLICO\n",
      "M.P. C/ AURELIO NUÑEZ VELAZQUEZ Y CLEMENTE NUÑEZ VELAZQUEZ SI\n",
      "HOMICIDIO DOLOSO\n",
      "ACUERDO Y SENTENCIA N° 13\n",
      "30 de abril de 2010\n",
      "A. B. PUCHETA de CORREA\n",
      "S. BLANCO\n",
      "L. M. BENITEZ RIERA\n",
      "L. M. BENITEZ RIERA\n",
      "ALICIA PUCHETA de CORREA\n",
      "SINDULFO BLANCO\n",
      "SINDULFO BLANCO\n",
      "L. M. BENITEZ RIERA\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLANCO\n",
      "ALICIA PUCHETA de CORREA\n",
      "L. M. BENITEZ RIERA\n",
      "SINDULFO BLAN\n"
     ]
    }
   ],
   "source": [
    "result = pipeline(\"Re-escribe el siguiente texto jurídico con el objetivo de limpiar los errores de OCR mientras mantienes el contenido original en español:\\n\\n\" + text.text,\n",
    "         max_new_tokens=1024)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
